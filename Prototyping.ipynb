{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document():\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.vecs = []\n",
    "        self.weights = []\n",
    "        self.weights_sum = 0\n",
    "    \n",
    "    def setvecs(self, model):\n",
    "        for i, w in enumerate(self.words):\n",
    "            self.vecs.append(model[w])\n",
    "    \n",
    "    def setweights(self):\n",
    "        total, counts = self._getcounts(self.words)\n",
    "        for w in self.words:\n",
    "            self.weights.append(counts[w] / total)\n",
    "        self.weights_sum = sum(self.weights)\n",
    "    \n",
    "    def getweight(self, word):\n",
    "        if word in self.words:\n",
    "            return self.weights[self.words.index(word)]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def getvec(self, word):\n",
    "        if word in self.words:\n",
    "            return self.vecs[self.words.index(word)]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _getcounts(self, words):\n",
    "        counts = Counter(words)\n",
    "        total = sum(counts.values())\n",
    "        return total, counts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocPair():\n",
    "    def __init__(self, doc1, doc2):\n",
    "        self.doc1 = doc1\n",
    "        self.doc2 = doc2\n",
    "        self.vecs = []\n",
    "    \n",
    "    def _getvocab(self):\n",
    "        vocab = list(set(self.doc1.words + self.doc2.words))\n",
    "        print(vocab)\n",
    "        return len(vocab), vocab\n",
    "    \n",
    "    def getsignature(self):\n",
    "        n, vocab = self._getvocab()\n",
    "        self.sig1 = [self.doc1.getweight(w) if w in self.doc1.words else 0.0 for w in vocab]\n",
    "        self.sig2 = [self.doc2.getweight(w) if w in self.doc2.words  else 0.0 for w in vocab]\n",
    "        \n",
    "    def getvecs(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.weights[doc1.words.index('speaks')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['illinois', 'speaks', 'obama', 'media']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in vocab if w in doc1.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc1.getweight(w) if w in doc1.words else 0.0 for w in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['illinois', 'speaks', 'president', 'greets', 'obama', 'press', 'media', 'chicago']\n"
     ]
    }
   ],
   "source": [
    "docpair = DocPair(doc1, doc2)\n",
    "docpair.getsignature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docpair.sig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list([1,2,3])\n",
    "test.extend([4,5])\n",
    "set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_1w_corpus(name, sep=\"\\t\"):\n",
    "    for line in open(name):\n",
    "        yield line.split(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "vocabulary = set(model.vocab)\n",
    "relevant_words = [word for (word, count) in read_1w_corpus('count_1w.txt') if word in vocabulary]\n",
    "model_reduced = model[[w for w in relevant_words]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.25 s, sys: 4.25 s, total: 7.5 s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%time model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obama speaks to the media in Illinois \n",
    "sentence1_words = ['obama', 'speaks', 'media', 'illinois']\n",
    "\n",
    "# The President greets the press in Chicago. \n",
    "sentence2_words = ['president', 'greets', 'press', 'chicago']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = Document(sentence1_words)\n",
    "doc1.setvecs(model)\n",
    "doc1.setweights()\n",
    "\n",
    "doc2 = Document(sentence2_words)\n",
    "doc2.setvecs(model)\n",
    "doc2.setweights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mikaelbrunila/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "doc1 = \" \".join(sentence1_words)\n",
    "doc2 = \" \".join(sentence2_words)\n",
    "nbow = CountVectorizer(stop_words = sw)\n",
    "nbow.fit([doc for doc in [doc1, doc2]])\n",
    "\n",
    "vocabulary = set(model.index2word)\n",
    "names = nbow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2 = nbow.transform([doc1, doc2])   \n",
    "index = np.union1d(v1.indices, v2.indices)\n",
    "\n",
    "v1  = v1.toarray().ravel()\n",
    "v2  = v2.toarray().ravel()\n",
    "\n",
    "n = len(index) \n",
    "index_map = [(index[i], i) for i in range(n) if names[index[i]] in vocabulary]\n",
    "source    = np.zeros(n)\n",
    "sink      = np.zeros(n)\n",
    "vecs      = np.zeros(shape = (n, 300))\n",
    "    \n",
    "for i, j in index_map:\n",
    "    source[j] = v1[i]\n",
    "    sink[j]   = v2[i]\n",
    "    vecs[j]   = model[names[i]]\n",
    "\n",
    "sum_source = sum(source)\n",
    "sum_sink = sum(sink)\n",
    "if sum_source == 0:\n",
    "    sum_source = 1\n",
    "if sum_sink == 0:\n",
    "    sum_sink = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.25, 0.25, 0.25, 0.  , 0.  , 0.25])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source / sum_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 300)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-fwmd",
   "language": "python",
   "name": "venv-fwmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

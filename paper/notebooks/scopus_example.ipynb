{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from flow_wmd.documents import Document\n",
    "from flow_wmd.models import LC_RWMD, WMD, WMDManyToMany\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Source title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>discipline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uzzi B.</td>\n",
       "      <td>1996</td>\n",
       "      <td>American Sociological Review</td>\n",
       "      <td>In this paper, I attempt to advance the concep...</td>\n",
       "      <td>soc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inglehart R., Baker W.E.</td>\n",
       "      <td>2000</td>\n",
       "      <td>American Sociological Review</td>\n",
       "      <td>Modernization theorists from Karl Marx to Dani...</td>\n",
       "      <td>soc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Burt R.S.</td>\n",
       "      <td>2004</td>\n",
       "      <td>American Journal of Sociology</td>\n",
       "      <td>This article outlines the mechanism by which b...</td>\n",
       "      <td>soc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meyer J.W., Boli J., Thomas G.M., Ramirez F.O.</td>\n",
       "      <td>1997</td>\n",
       "      <td>American Journal of Sociology</td>\n",
       "      <td>The authors analyze the nation-state as a worl...</td>\n",
       "      <td>soc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emirbayer M., Mische A.</td>\n",
       "      <td>1998</td>\n",
       "      <td>American Journal of Sociology</td>\n",
       "      <td>This article aims (1) to analytically disaggre...</td>\n",
       "      <td>soc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Authors  Year  \\\n",
       "0                                         Uzzi B.  1996   \n",
       "1                        Inglehart R., Baker W.E.  2000   \n",
       "2                                       Burt R.S.  2004   \n",
       "3  Meyer J.W., Boli J., Thomas G.M., Ramirez F.O.  1997   \n",
       "4                         Emirbayer M., Mische A.  1998   \n",
       "\n",
       "                    Source title  \\\n",
       "0   American Sociological Review   \n",
       "1   American Sociological Review   \n",
       "2  American Journal of Sociology   \n",
       "3  American Journal of Sociology   \n",
       "4  American Journal of Sociology   \n",
       "\n",
       "                                            Abstract discipline  \n",
       "0  In this paper, I attempt to advance the concep...        soc  \n",
       "1  Modernization theorists from Karl Marx to Dani...        soc  \n",
       "2  This article outlines the mechanism by which b...        soc  \n",
       "3  The authors analyze the nation-state as a worl...        soc  \n",
       "4  This article aims (1) to analytically disaggre...        soc  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc = pd.read_csv(\"soc.csv\",low_memory=False)\n",
    "soc[\"discipline\"] = \"soc\"\n",
    "econ = pd.read_csv(\"econ.csv\",low_memory=False)\n",
    "econ[\"discipline\"] = \"econ\"\n",
    "\n",
    "scopus = pd.concat([soc,econ])\n",
    "\n",
    "stopword_list=stopwords.words('english')\n",
    "\n",
    "scopus.drop([i for i in scopus.columns if \"Unnamed\" in i],axis=1,inplace=True)\n",
    "scopus.reset_index(inplace=True,drop=True)\n",
    "\n",
    "scopus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize cleanup functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to remove noisy formatting, lemmatizing and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing functions\n",
    "# Partly self-authored, partly from https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = re.sub('<br / ><br / >', ' ', text)\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Lemmatizing the text\n",
    "def simple_lemmatizer(text):\n",
    "    lemmatizer=WordNetLemmatizer() \n",
    "    text= ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, stopword_list, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove special formatting and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords before denoising, lemmatizing and removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "tokenizer=ToktokTokenizer()\n",
    "scopus['Abstract_clean']= [remove_stopwords(r, stopword_list) for r in scopus['Abstract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoise, remove special characters, lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "scopus['Abstract_clean']=scopus['Abstract_clean'].apply(denoise_text)\n",
    "scopus['Abstract_clean']=scopus['Abstract_clean'].apply(remove_special_characters)\n",
    "scopus['Abstract_clean']=scopus['Abstract_clean'].apply(simple_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords again, after other preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "scopus['Abstract_clean']= [remove_stopwords(r, stopword_list) for r in scopus['Abstract_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data _before_ preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this paper, I attempt to advance the concept of embeddedness beyond the level of a programmatic statement by developing a formulation that specifies how embeddedness and network structure affect economic action. On the basis of existing theory and original ethnographies of 23 apparel firms, I develop a systematic scheme that more fully demarcates the unique features, functions, and sources of embeddedness. From this scheme, I derive a set of refutable implications and test their plausibility, using another data set on the network ties of all better dress apparel firms in the New York apparel economy. Results reveal that embeddedness is an exchange system with unique opportunities relative to markets and that firms organized in networks have higher survival chances than do firms which maintain arm's-length market relationships. The positive effect of embeddedness reaches a threshold, however, after which point the positive effect reverses itself.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scopus['Abstract'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data _after_ preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper attempt advance concept embeddedness beyond level programmatic statement developing formulation specifies embeddedness network structure affect economic action basis existing theory original ethnography apparel firm develop systematic scheme fully demarcates unique feature function source embeddedness scheme derive set refutable implication test plausibility using another data set network tie better dress apparel firm new york apparel economy result reveal embeddedness exchange system unique opportunity relative market firm organized network higher survival chance firm maintain arm slength market relationship positive effect embeddedness reach threshold however point positive effect revers'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scopus['Abstract_clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Separate pos and neg reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc = scopus[scopus.discipline == \"soc\"].reset_index(drop=True)\n",
    "econ = scopus[scopus.discipline == \"econ\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc = soc.Abstract_clean.tolist()\n",
    "econ = econ.Abstract_clean.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenize and \"sample\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "soc_tok = list(map(tokenize, soc))\n",
    "econ_tok = list(map(tokenize, econ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_sample = [\" \".join(doc) for doc in soc_tok]\n",
    "econ_sample = [\" \".join(doc) for doc in econ_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load pretrained Google News W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n",
      "CPU times: user 53.6 s, sys: 3.42 s, total: 57.1 s\n",
      "Wall time: 58 s\n"
     ]
    }
   ],
   "source": [
    "def read_1w_corpus(name, sep=\"\\t\"):\n",
    "    for line in open(name):\n",
    "        yield line.split(sep)\n",
    "\n",
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('/Users/jack/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load corpus and remove OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54 µs, sys: 182 µs, total: 236 µs\n",
      "Wall time: 249 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 926 ms, sys: 1.44 s, total: 2.36 s\n",
      "Wall time: 5.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l1', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenize at 0x1a230e7560>, use_idf=False,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = soc_sample + econ_sample\n",
    "\n",
    "%time vectorizer = TfidfVectorizer(use_idf=False, tokenizer=tokenize, norm='l1')\n",
    "%time vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 58.1 ms, total: 89.2 ms\n",
      "Wall time: 91 ms\n"
     ]
    }
   ],
   "source": [
    "%time oov = [word for word in vectorizer.get_feature_names() if word not in model.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3692"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.73 s, sys: 99.8 ms, total: 7.83 s\n",
      "Wall time: 7.97 s\n",
      "CPU times: user 3.28 s, sys: 35.1 ms, total: 3.31 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "#removing the oov words\n",
    "def remove_oov(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in oov]\n",
    "    #filtered_tokens = filter(lambda token: token not in oov, tokens)\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "%time soc_sample = list(map(remove_oov, soc_sample))\n",
    "%time econ_sample = list(map(remove_oov, econ_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper attempt advance concept embeddedness beyond level programmatic statement developing formulation specifies embeddedness network structure affect economic action basis existing theory original ethnography apparel firm develop systematic scheme fully demarcates unique feature function source embeddedness scheme derive set refutable implication test plausibility using another data set network tie better dress apparel firm new york apparel economy result reveal embeddedness exchange system unique opportunity relative market firm organized network higher survival chance firm maintain arm market relationship positive effect embeddedness reach threshold however point positive effect revers'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 µs, sys: 13 µs, total: 36 µs\n",
      "Wall time: 38.9 µs\n",
      "CPU times: user 548 ms, sys: 21.2 ms, total: 570 ms\n",
      "Wall time: 656 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l1', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenize at 0x1a230e7560>, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = soc_sample + econ_sample\n",
    "\n",
    "%time vectorizer = TfidfVectorizer(use_idf=True, tokenizer=tokenize,norm='l1')\n",
    "%time vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "soc_nbow = vectorizer.transform(soc_sample)\n",
    "econ_nbow = vectorizer.transform(econ_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_tok = list(map(tokenize, soc_sample))\n",
    "econ_tok =  list(map(tokenize, econ_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper',\n",
       " 'attempt',\n",
       " 'advance',\n",
       " 'concept',\n",
       " 'embeddedness',\n",
       " 'beyond',\n",
       " 'level',\n",
       " 'programmatic',\n",
       " 'statement',\n",
       " 'developing',\n",
       " 'formulation',\n",
       " 'specifies',\n",
       " 'embeddedness',\n",
       " 'network',\n",
       " 'structure',\n",
       " 'affect',\n",
       " 'economic',\n",
       " 'action',\n",
       " 'basis',\n",
       " 'existing']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_tok[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 ms, sys: 650 µs, total: 18.2 ms\n",
      "Wall time: 18.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time oov_ = [word for word in vectorizer.get_feature_names() if word not in model.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2829"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Get features and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "idx2word = {idx: word for idx, word in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the embedding matrix \"E\" for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "E = np.vstack([model.word_vec(word) for word in vectorizer.get_feature_names()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Initialize documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all reviews into \"documents\", each with a set of weights per word in the corpus (\"nbow\"), the sum of these weights (\"weights_sum\"), the indeces of the words in the documents (\"idxs\") and the word vectors corresponding to each word (\"vecs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 14.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "soc_docs, econ_docs = [], []\n",
    "\n",
    "for idx, doc in enumerate(soc_tok):\n",
    "    soc_docs.append(Document(doc, soc_nbow[idx], word2idx, E))\n",
    "    \n",
    "for idx, doc in enumerate(econ_tok):\n",
    "    econ_docs.append(Document(doc, econ_nbow[idx], word2idx, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_docs[0].nbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_docs[0].weights_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4480, 4227, 3460, 260, 10629, 2692, 3848, 777, 8844, 141]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_docs[0].idxs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12792969, -0.10839844, -0.13476562,  0.07568359, -0.25585938,\n",
       "       -0.12402344,  0.13183594, -0.11767578, -0.08154297, -0.1015625 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soc_docs[0].vecs[:1][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Linear-Complexity Relaxed WMD (LC-RWMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the [Linear-Complexity Relaxed WMD](https://arxiv.org/abs/1711.07227) to get the distances between all positive and all negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 200 µs, total: 219 µs\n",
      "Wall time: 995 µs\n",
      "CPU times: user 11min 45s, sys: 1min 41s, total: 13min 26s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%time lc_rwmd = LC_RWMD(soc_docs, econ_docs,soc_nbow,econ_nbow,E)\n",
    "%time lc_rwmd.get_D()\n",
    "#%time lc_rwmd.get_L(1)\n",
    "#%time lc_rwmd.get_rwmd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Gale-Shapeley Pairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Gale-Shapeley matching algorithm](https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm) to find the optimal pairs between positive and negative reviews. This iterates over all the reviews and finds the set of matches that pairs each review with its optimal match given that all positive reviews have to be matched with a negative review and vice versa. The output is a dictionary of key-value pairs, where each pair represents an optimal match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_wmd.gale_shapeley import Matcher\n",
    "\n",
    "matcher = Matcher(lc_rwmd.D)\n",
    "engaged = matcher.matchmaker()\n",
    "matcher.check()\n",
    "pairs = engaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output of Gale-Shapeley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1009, 148),\n",
       " (323, 510),\n",
       " (1643, 563),\n",
       " (1348, 100),\n",
       " (786, 1000),\n",
       " (1247, 202),\n",
       " (134, 1002),\n",
       " (19, 196),\n",
       " (675, 1004),\n",
       " (1669, 356)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "\n",
    "take(10, pairs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Pairwise WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pairwise distances between the documents selected by the Galey-Shapeley algorithm _without_ returning the flow between individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances between 0 documents.\n",
      "Calculated distances between 100 documents.\n",
      "Calculated distances between 200 documents.\n",
      "Calculated distances between 300 documents.\n",
      "Calculated distances between 400 documents.\n",
      "Calculated distances between 500 documents.\n",
      "Calculated distances between 600 documents.\n",
      "Calculated distances between 700 documents.\n",
      "Calculated distances between 800 documents.\n",
      "Calculated distances between 900 documents.\n",
      "Calculated distances between 1000 documents.\n",
      "Calculated distances between 1100 documents.\n",
      "CPU times: user 1min 28s, sys: 1.65 s, total: 1min 29s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "from flow_wmd.models import WMDPairs\n",
    "\n",
    "wmd_pairs = WMDPairs(soc_docs,econ_docs,pairs,E,idx2word)\n",
    "%time wmd_pairwise = wmd_pairs.get_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is a matrix of distances between the document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pairwise distances between the documents selected by the Galey-Shapeley algorithm, this time also returning the flow between individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances between 0 documents.\n",
      "Calculated distances between 100 documents.\n",
      "Calculated distances between 200 documents.\n",
      "Calculated distances between 300 documents.\n",
      "Calculated distances between 400 documents.\n",
      "Calculated distances between 500 documents.\n",
      "Calculated distances between 600 documents.\n",
      "Calculated distances between 700 documents.\n",
      "Calculated distances between 800 documents.\n",
      "Calculated distances between 900 documents.\n",
      "Calculated distances between 1000 documents.\n",
      "Calculated distances between 1100 documents.\n",
      "CPU times: user 1min 32s, sys: 1.14 s, total: 1min 33s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "wmd_pairs_flow = WMDPairs(soc_docs,econ_docs,pairs,E,idx2word)\n",
    "%time wmd_pairwise_flow = wmd_pairs_flow.get_distances(return_flow = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three return values.\n",
    "\n",
    "The first one is again a matrix of distances between the document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairwise_flow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second return value is a list of tuples with all the words that contributed the most to the distance from the positive documents to the negative ones. These are _not_ sorted from high to low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deprived', 0),\n",
       " ('disaggregates', 0.062299999999999994),\n",
       " ('tit', 0.06724),\n",
       " ('neutralize', 0.10158),\n",
       " ('properly', 0.10774),\n",
       " ('bill', 0.27364),\n",
       " ('larceny', 0),\n",
       " ('mulatto', 0.24312999999999999),\n",
       " ('refresh', 0.09245),\n",
       " ('randomness', 0.18383)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairwise_flow[1].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third return value is a list of tuples with all the words that contributed the most to the distance from the negative documents to the positive ones. Again, these are _not_ sorted from high to low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('needy', 0.10522999999999999),\n",
       " ('properly', 0.14555),\n",
       " ('bill', 0.37583999999999995),\n",
       " ('labile', 0.11868000000000001),\n",
       " ('randomness', 0.061380000000000004),\n",
       " ('motivate', 0.10601000000000001),\n",
       " ('monte', 0.20939),\n",
       " ('hunting', 0.11146),\n",
       " ('initiate', 0.23603000000000002),\n",
       " ('schooling', 2.13311)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairwise_flow[2].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Intepreting pairwise WMD flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sort the distances of the words that created the most distance from the positive to the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'social': 17.552079999999993,\n",
       " 'woman': 12.597589999999997,\n",
       " 'author': 10.976540000000005,\n",
       " 'inequality': 10.68216,\n",
       " 'network': 9.804319999999999,\n",
       " 'effect': 9.714220000000001,\n",
       " 'article': 9.640240000000002,\n",
       " 'analysis': 8.398250000000003,\n",
       " 'gender': 8.362820000000003,\n",
       " 'status': 8.228179999999998,\n",
       " 'relationship': 8.034370000000001,\n",
       " 'data': 7.790869999999999,\n",
       " 'study': 7.78603,\n",
       " 'theory': 7.683050000000004,\n",
       " 'economic': 7.498829999999997,\n",
       " 'black': 7.461530000000001,\n",
       " 'research': 7.283319999999998,\n",
       " 'state': 7.230560000000003,\n",
       " 'organization': 7.128590000000001,\n",
       " 'family': 7.086600000000001,\n",
       " 'survey': 7.0046500000000025,\n",
       " 'segregation': 6.992670000000001,\n",
       " 'political': 6.964210000000001,\n",
       " 'chicago': 6.962130000000001,\n",
       " 'child': 6.79948,\n",
       " 'racial': 6.75736,\n",
       " 'structure': 6.698240000000004,\n",
       " 'men': 6.6356600000000014,\n",
       " 'white': 6.4816999999999965,\n",
       " 'labor': 6.344799999999999}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wmd_pairwise_flow[1].items(), key=lambda item: item[1], reverse=True)[:30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what added most distance when moving from the negative to the positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>cumsum</th>\n",
       "      <th>cum_perc</th>\n",
       "      <th>cumcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.26953</td>\n",
       "      <td>17.26953</td>\n",
       "      <td>0.511908</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.38263</td>\n",
       "      <td>31.65216</td>\n",
       "      <td>0.938241</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.08137</td>\n",
       "      <td>44.73353</td>\n",
       "      <td>1.326003</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.97264</td>\n",
       "      <td>57.70617</td>\n",
       "      <td>1.710541</td>\n",
       "      <td>0.000587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.87137</td>\n",
       "      <td>69.57754</td>\n",
       "      <td>2.062435</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    cumsum  cum_perc  cumcount\n",
       "0  17.26953  17.26953  0.511908  0.000147\n",
       "1  14.38263  31.65216  0.938241  0.000294\n",
       "2  13.08137  44.73353  1.326003  0.000440\n",
       "3  12.97264  57.70617  1.710541  0.000587\n",
       "4  11.87137  69.57754  2.062435  0.000734"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vals = {k: v for k, v in sorted(wmd_pairwise_flow[2].items(), key=lambda item: item[1], reverse=True)}.values()\n",
    "\n",
    "v = pd.DataFrame(vals)\n",
    "v[\"cumsum\"] = v[0].cumsum()\n",
    "v['cum_perc'] = 100*v['cumsum']/v[0].sum()\n",
    "#v.loc[640,\"cum_perc\"]\n",
    "v[\"cumcount\"] = [(i+1) / len(v) for i in range(len(v))]\n",
    "v.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6811"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543385699603583"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.index[6500]/len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b2df72e50>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(v.index,v[\"cum_perc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Many-to-many WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a first attempt to do the flows from words between many documents, without first filtering using Gale-Shapeley. However, this proved too inefficient. As you can see looking at the CPU times, it is very slow even with extremely small samples and the time complexity is quadratic (or worse?), meaning it rapidly gets even worse as the sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 1.92 s, total: 2min 18s\n",
      "Wall time: 54.3 s\n"
     ]
    }
   ],
   "source": [
    "%time m2m_distances = WMDManyToMany(pos_docs[:20], neg_docs[:20],E,idx2word).get_distances(return_flow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 1.9 s, total: 2min 24s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%time m2m_distances_flow, wc_X1, wc_X2 = WMDManyToMany(pos_docs[:20],neg_docs[:20],E,idx2word).get_distances(return_flow = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'karen': 8.69223,\n",
       " 'wrenching': 8.31882,\n",
       " 'carpenter': 7.468960000000001,\n",
       " 'laughter': 7.467879999999999,\n",
       " 'liked': 6.864090000000003,\n",
       " 'mom': 6.791519999999999,\n",
       " 'gut': 6.759419999999999,\n",
       " 'love': 6.551409999999997,\n",
       " 'camp': 6.533080000000001,\n",
       " 'hr': 6.1393699999999995}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wc_X1.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hopper': 8.372459999999998,\n",
       " 'jake': 7.63837,\n",
       " 'movie': 7.267059999999995,\n",
       " 'film': 6.936379999999998,\n",
       " 'shakespeare': 5.99276,\n",
       " 'oddness': 5.53033,\n",
       " 'terrible': 4.943440000000001,\n",
       " 'parent': 4.751790000000001,\n",
       " 'actor': 4.672620000000001,\n",
       " 'bad': 4.430020000000002}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wc_X2.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.97848431809601"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9881"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

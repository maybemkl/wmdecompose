{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from flow_wmd.documents import Document\n",
    "from flow_wmd.models import LC_RWMD, WMD, WMDManyToMany\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl = pd.read_csv(\"/Users/jack/Desktop/benchmarking datasets/reddit_sports/nfl.csv\")\n",
    "nba =  pd.read_csv(\"/Users/jack/Desktop/benchmarking datasets/reddit_sports/nba.csv\")\n",
    "df = pd.concat([nfl,nba]).reset_index(drop=True)\n",
    "stopword_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize cleanup functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to remove noisy formatting, lemmatizing and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing functions\n",
    "# Partly self-authored, partly from https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = re.sub('<br / ><br / >', ' ', text)\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Lemmatizing the text\n",
    "def simple_lemmatizer(text):\n",
    "    lemmatizer=WordNetLemmatizer() \n",
    "    text= ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, stopword_list, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove special formatting and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords before denoising, lemmatizing and removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "tokenizer=ToktokTokenizer()\n",
    "df['body_clean']= [remove_stopwords(r, stopword_list) for r in df['body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoise, remove special characters, lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "df['body_clean']=df['body_clean'].apply(denoise_text)\n",
    "df['body_clean']=df['body_clean'].apply(remove_special_characters)\n",
    "df['body_clean']=df['body_clean'].apply(simple_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords again, after other preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "df['body_clean']= [remove_stopwords(r, stopword_list) for r in df['body_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove reddit formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redditcleaner import clean\n",
    "\n",
    "df['body_clean']=[clean(body) for body in df[\"body_clean\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data _before_ preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yeah, and I think he got the offer (although never confirmed), but turned it down. Certainly gonna be a concern he leaves us in the coming years for the college game but that is a lot different (and less enticing) switch than being poached by an NFL team to be their HC. \\n\\nWhiz seriously would do well as a college coach though. I just hope he prefers the Chargers. We all bitch about how our offense is run sometimes, but he is at the worst an above average OC that offers unique consistency in staying with the team for long periods of time. A lot of our problems are unavoidable as well, considering our OL is a bunch of fucking goons that require an assistant OL coach position just to make sure everyone’s shoes are tied. It’s gotta be a pain in the ass scheming around Dan “fuck pass protection” Feeney when the Rams are shoving Suh and Donald down your throat every play.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data _after_ preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah think got offer although never confirmed turned certainly gonna concern leaf u coming year college game lot different le enticing switch poached nfl team hc whiz seriously would well college coach though hope prefers charger bitch offense run sometimes worst average oc offer unique consistency staying team long period time lot problem unavoidable well considering ol bunch fucking goon require assistant ol coach position make sure everyone shoe tied gotta pain scheming around dan fuck pas protection feeney ram shoving suh donald throat every play'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body_clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Separate pos and neg reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl = df[df.subreddit == \"nfl\"].reset_index(drop=True)\n",
    "nba = df[df.subreddit == \"nba\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl = nfl.body_clean.tolist()\n",
    "nba = nba.body_clean.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenize and \"sample\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "nfl_tok = list(map(tokenize, nfl))\n",
    "nba_tok = list(map(tokenize, nba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_sample = [\" \".join(doc) for doc in nfl_tok]\n",
    "nba_sample = [\" \".join(doc) for doc in nba_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load pretrained Google News W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n",
      "CPU times: user 54.8 s, sys: 7.17 s, total: 1min 1s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "def read_1w_corpus(name, sep=\"\\t\"):\n",
    "    for line in open(name):\n",
    "        yield line.split(sep)\n",
    "\n",
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('/Users/jack/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load corpus and remove OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69 µs, sys: 758 µs, total: 827 µs\n",
      "Wall time: 2.18 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 2.26 s, total: 3.58 s\n",
      "Wall time: 8.63 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l1', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenize at 0x1a18ba6b00>, use_idf=False,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = nfl_sample + nba_sample\n",
    "\n",
    "%time vectorizer = TfidfVectorizer(use_idf=False, tokenizer=tokenize, norm='l1')\n",
    "%time vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.1 ms, sys: 157 ms, total: 215 ms\n",
      "Wall time: 340 ms\n"
     ]
    }
   ],
   "source": [
    "%time oov = [word for word in vectorizer.get_feature_names() if word not in model.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6508"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 223 ms, total: 21.3 s\n",
      "Wall time: 21.6 s\n",
      "CPU times: user 19.6 s, sys: 180 ms, total: 19.8 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "#removing the oov words\n",
    "def remove_oov(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in oov]\n",
    "    #filtered_tokens = filter(lambda token: token not in oov, tokens)\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "%time nfl_sample = list(map(remove_oov, nfl_sample))\n",
    "%time nba_sample = list(map(remove_oov, nba_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove documents that are empty after removing stopwords and OOV\n",
    "\n",
    "for c,i in enumerate(nba_sample):\n",
    "    l = i.split()\n",
    "    if len(l)==0:\n",
    "        del nba_sample[c]\n",
    "        \n",
    "for c,i in enumerate(nfl_sample):\n",
    "    l = i.split()\n",
    "    if len(l)==0:\n",
    "        del nfl_sample[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 µs, sys: 601 µs, total: 712 µs\n",
      "Wall time: 1.37 ms\n",
      "CPU times: user 1.03 s, sys: 52 ms, total: 1.08 s\n",
      "Wall time: 1.14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l1', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenize at 0x1a18ba6b00>, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = nfl_sample + nba_sample\n",
    "\n",
    "%time vectorizer = TfidfVectorizer(use_idf=True, tokenizer=tokenize,norm='l1')\n",
    "%time vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "nfl_nbow = vectorizer.transform(nfl_sample)\n",
    "nba_nbow = vectorizer.transform(nba_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_tok = list(map(tokenize, nfl_sample))\n",
    "nba_tok =  list(map(tokenize, nba_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah',\n",
       " 'think',\n",
       " 'got',\n",
       " 'offer',\n",
       " 'although',\n",
       " 'never',\n",
       " 'confirmed',\n",
       " 'turned',\n",
       " 'certainly',\n",
       " 'gonna',\n",
       " 'concern',\n",
       " 'leaf',\n",
       " 'u',\n",
       " 'coming',\n",
       " 'year',\n",
       " 'college',\n",
       " 'game',\n",
       " 'lot',\n",
       " 'different',\n",
       " 'le']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_tok[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.8 ms, sys: 214 ms, total: 269 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%time oov_ = [word for word in vectorizer.get_feature_names() if word not in model.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Get features and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "idx2word = {idx: word for idx, word in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the embedding matrix \"E\" for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "E = np.vstack([model.word_vec(word) for word in vectorizer.get_feature_names()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Initialize documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all reviews into \"documents\", each with a set of weights per word in the corpus (\"nbow\"), the sum of these weights (\"weights_sum\"), the indeces of the words in the documents (\"idxs\") and the word vectors corresponding to each word (\"vecs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "nfl_docs, nba_docs = [], []\n",
    "\n",
    "for idx, doc in enumerate(nfl_tok):\n",
    "    nfl_docs.append(Document(doc, nfl_nbow[idx], word2idx, E))\n",
    "    \n",
    "for idx, doc in enumerate(nba_tok):\n",
    "    nba_docs.append(Document(doc, nba_nbow[idx], word2idx, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_docs[0].nbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_docs[0].weights_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6670, 15889, 1049, 6684, 11804, 3106, 16422, 6695, 6698, 15922]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_docs[0].idxs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26367188,  0.07568359,  0.16699219,  0.29101562,  0.04443359,\n",
       "       -0.06835938, -0.06591797, -0.04614258,  0.13574219,  0.16113281],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_docs[0].vecs[:1][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Linear-Complexity Relaxed WMD (LC-RWMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the [Linear-Complexity Relaxed WMD](https://arxiv.org/abs/1711.07227) to get the distances between all positive and all negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nfl_sample:\n",
    "    if len(i.split())==0:       \n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 µs, sys: 348 µs, total: 369 µs\n",
      "Wall time: 1.66 ms\n",
      "CPU times: user 2h 24min 11s, sys: 49min 46s, total: 3h 13min 58s\n",
      "Wall time: 58min 7s\n"
     ]
    }
   ],
   "source": [
    "%time lc_rwmd = LC_RWMD(nfl_docs, nba_docs,nfl_nbow,nba_nbow,E)\n",
    "%time lc_rwmd.get_D()\n",
    "#%time lc_rwmd.get_L(1)\n",
    "#%time lc_rwmd.get_rwmd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Gale-Shapeley Pairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Gale-Shapeley matching algorithm](https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm) to find the optimal pairs between positive and negative reviews. This iterates over all the reviews and finds the set of matches that pairs each review with its optimal match given that all positive reviews have to be matched with a negative review and vice versa. The output is a dictionary of key-value pairs, where each pair represents an optimal match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_wmd.gale_shapeley import Matcher\n",
    "\n",
    "matcher = Matcher(lc_rwmd.D)\n",
    "engaged = matcher.matchmaker()\n",
    "matcher.check()\n",
    "pairs = engaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output of Gale-Shapeley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4497, 0),\n",
       " (2364, 1),\n",
       " (1962, 3119),\n",
       " (4245, 100),\n",
       " (2007, 4184),\n",
       " (4000, 965),\n",
       " (3508, 2964),\n",
       " (977, 53),\n",
       " (533, 19),\n",
       " (3792, 3675)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "\n",
    "take(10, pairs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Pairwise WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pairwise distances between the documents selected by the Galey-Shapeley algorithm _without_ returning the flow between individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances between 0 documents.\n",
      "Calculated distances between 100 documents.\n",
      "Calculated distances between 200 documents.\n",
      "Calculated distances between 300 documents.\n",
      "Calculated distances between 400 documents.\n",
      "Calculated distances between 500 documents.\n",
      "Calculated distances between 600 documents.\n",
      "Calculated distances between 700 documents.\n",
      "Calculated distances between 800 documents.\n",
      "Calculated distances between 900 documents.\n",
      "Calculated distances between 1000 documents.\n",
      "Calculated distances between 1100 documents.\n",
      "Calculated distances between 1200 documents.\n",
      "Calculated distances between 1300 documents.\n",
      "Calculated distances between 1400 documents.\n",
      "Calculated distances between 1500 documents.\n",
      "Calculated distances between 1600 documents.\n",
      "Calculated distances between 1700 documents.\n",
      "Calculated distances between 1800 documents.\n",
      "Calculated distances between 1900 documents.\n",
      "Calculated distances between 2000 documents.\n",
      "Calculated distances between 2100 documents.\n",
      "Calculated distances between 2200 documents.\n",
      "Calculated distances between 2300 documents.\n",
      "Calculated distances between 2400 documents.\n",
      "Calculated distances between 2500 documents.\n",
      "Calculated distances between 2600 documents.\n",
      "Calculated distances between 2700 documents.\n",
      "Calculated distances between 2800 documents.\n",
      "Calculated distances between 2900 documents.\n",
      "Calculated distances between 3000 documents.\n",
      "Calculated distances between 3100 documents.\n",
      "Calculated distances between 3200 documents.\n",
      "Calculated distances between 3300 documents.\n",
      "Calculated distances between 3400 documents.\n",
      "Calculated distances between 3500 documents.\n",
      "Calculated distances between 3600 documents.\n",
      "Calculated distances between 3700 documents.\n",
      "Calculated distances between 3800 documents.\n",
      "Calculated distances between 3900 documents.\n",
      "Calculated distances between 4000 documents.\n",
      "Calculated distances between 4100 documents.\n",
      "Calculated distances between 4200 documents.\n",
      "Calculated distances between 4300 documents.\n",
      "Calculated distances between 4400 documents.\n",
      "Calculated distances between 4500 documents.\n",
      "Calculated distances between 4600 documents.\n",
      "Calculated distances between 4700 documents.\n",
      "Calculated distances between 4800 documents.\n",
      "Calculated distances between 4900 documents.\n",
      "CPU times: user 4min 6s, sys: 5.14 s, total: 4min 11s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "from flow_wmd.models import WMDPairs\n",
    "\n",
    "wmd_pairs = WMDPairs(nfl_docs,nba_docs,pairs,E,idx2word)\n",
    "%time wmd_pairwise = wmd_pairs.get_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is a matrix of distances between the document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pairwise distances between the documents selected by the Galey-Shapeley algorithm, this time also returning the flow between individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances between 0 documents.\n",
      "Calculated distances between 100 documents.\n",
      "Calculated distances between 200 documents.\n",
      "Calculated distances between 300 documents.\n",
      "Calculated distances between 400 documents.\n",
      "Calculated distances between 500 documents.\n",
      "Calculated distances between 600 documents.\n",
      "Calculated distances between 700 documents.\n",
      "Calculated distances between 800 documents.\n",
      "Calculated distances between 900 documents.\n",
      "Calculated distances between 1000 documents.\n",
      "Calculated distances between 1100 documents.\n",
      "Calculated distances between 1200 documents.\n",
      "Calculated distances between 1300 documents.\n",
      "Calculated distances between 1400 documents.\n",
      "Calculated distances between 1500 documents.\n",
      "Calculated distances between 1600 documents.\n",
      "Calculated distances between 1700 documents.\n",
      "Calculated distances between 1800 documents.\n",
      "Calculated distances between 1900 documents.\n",
      "Calculated distances between 2000 documents.\n",
      "Calculated distances between 2100 documents.\n",
      "Calculated distances between 2200 documents.\n",
      "Calculated distances between 2300 documents.\n",
      "Calculated distances between 2400 documents.\n",
      "Calculated distances between 2500 documents.\n",
      "Calculated distances between 2600 documents.\n",
      "Calculated distances between 2700 documents.\n",
      "Calculated distances between 2800 documents.\n",
      "Calculated distances between 2900 documents.\n",
      "Calculated distances between 3000 documents.\n",
      "Calculated distances between 3100 documents.\n",
      "Calculated distances between 3200 documents.\n",
      "Calculated distances between 3300 documents.\n",
      "Calculated distances between 3400 documents.\n",
      "Calculated distances between 3500 documents.\n",
      "Calculated distances between 3600 documents.\n",
      "Calculated distances between 3700 documents.\n",
      "Calculated distances between 3800 documents.\n",
      "Calculated distances between 3900 documents.\n",
      "Calculated distances between 4000 documents.\n",
      "Calculated distances between 4100 documents.\n",
      "Calculated distances between 4200 documents.\n",
      "Calculated distances between 4300 documents.\n",
      "Calculated distances between 4400 documents.\n",
      "Calculated distances between 4500 documents.\n",
      "Calculated distances between 4600 documents.\n",
      "Calculated distances between 4700 documents.\n",
      "Calculated distances between 4800 documents.\n",
      "Calculated distances between 4900 documents.\n",
      "CPU times: user 4min 26s, sys: 3.88 s, total: 4min 29s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "wmd_pairs_flow = WMDPairs(nfl_docs,nba_docs,pairs,E,idx2word)\n",
    "%time wmd_pairwise_flow = wmd_pairs_flow.get_distances(return_flow = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three return values.\n",
    "\n",
    "The first one is again a matrix of distances between the document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairwise_flow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second return value is a list of tuples with all the words that contributed the most to the distance from the positive documents to the negative ones. These are _not_ sorted from high to low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rife', 0.06325),\n",
       " ('trademark', 0.26766999999999996),\n",
       " ('boneheaded', 0.40813),\n",
       " ('piss', 1.5792100000000002),\n",
       " ('surprise', 2.8869199999999995),\n",
       " ('forgive', 0.8160400000000001),\n",
       " ('payday', 0.8864299999999999),\n",
       " ('approve', 0.15117),\n",
       " ('gifs', 0.35931),\n",
       " ('wilder', 0.00753)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairwise_flow[1].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third return value is a list of tuples with all the words that contributed the most to the distance from the negative documents to the positive ones. Again, these are _not_ sorted from high to low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blurt', 0.13701),\n",
       " ('soooooooo', 0.11983000000000002),\n",
       " ('trademark', 0.14214),\n",
       " ('hooper', 0.10486999999999999),\n",
       " ('piss', 1.89673),\n",
       " ('forgive', 0.8594999999999999),\n",
       " ('surprise', 0.99889),\n",
       " ('lapse', 0.64412),\n",
       " ('payday', 0.18817),\n",
       " ('approve', 0.1886)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairwise_flow[2].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Intepreting pairwise WMD flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sort the distances of the words that created the most distance from the positive to the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qb': 76.03633000000005,\n",
       " 'nfl': 58.08311000000005,\n",
       " 'yard': 46.122809999999994,\n",
       " 'year': 43.86639000000003,\n",
       " 'game': 35.08158999999997,\n",
       " 'brady': 30.907420000000023,\n",
       " 'team': 30.748360000000037,\n",
       " 'bowl': 30.237729999999996,\n",
       " 'season': 29.27642999999998,\n",
       " 'like': 29.181069999999945,\n",
       " 'think': 27.45017000000002,\n",
       " 'time': 27.15802999999999,\n",
       " 'get': 26.595749999999992,\n",
       " 'one': 26.522229999999976,\n",
       " 'would': 26.352930000000008,\n",
       " 'guy': 26.17762,\n",
       " 'good': 25.27174,\n",
       " 'people': 24.856300000000015,\n",
       " 'rb': 24.610240000000005,\n",
       " 'fan': 24.470229999999997,\n",
       " 'play': 24.416509999999985,\n",
       " 'last': 24.388399999999994,\n",
       " 'make': 23.79841999999999,\n",
       " 'wr': 23.184750000000005,\n",
       " 'football': 22.71535999999999,\n",
       " 'patriot': 22.631429999999998,\n",
       " 'offense': 22.417170000000002,\n",
       " 'better': 22.408880000000018,\n",
       " 'pat': 22.271230000000006,\n",
       " 'know': 21.930450000000015}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wmd_pairwise_flow[1].items(), key=lambda item: item[1], reverse=True)[:30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what added most distance when moving from the negative to the positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lebron': 76.02092999999998,\n",
       " 'player': 70.39909,\n",
       " 'harden': 64.42966999999996,\n",
       " 'team': 58.53596000000008,\n",
       " 'nba': 58.197859999999984,\n",
       " 'lakers': 57.88405999999996,\n",
       " 'game': 53.74635,\n",
       " 'playoff': 43.73188000000001,\n",
       " 'shot': 36.739230000000006,\n",
       " 'like': 36.65150999999996,\n",
       " 'season': 34.295340000000046,\n",
       " 'guy': 33.92289000000004,\n",
       " 'people': 33.804949999999984,\n",
       " 'point': 32.97387999999997,\n",
       " 'would': 32.75015999999999,\n",
       " 'warrior': 32.534659999999995,\n",
       " 'post': 31.95436,\n",
       " 'get': 31.729529999999976,\n",
       " 'kobe': 31.725779999999997,\n",
       " 'better': 31.20185,\n",
       " 'year': 31.03171999999998,\n",
       " 'think': 30.280169999999988,\n",
       " 'fan': 30.273599999999963,\n",
       " 'good': 29.319509999999998,\n",
       " 'kd': 29.26800999999999,\n",
       " 'play': 28.874340000000014,\n",
       " 'rocket': 27.734910000000003,\n",
       " 'league': 27.60889000000002,\n",
       " 'best': 27.60194000000001,\n",
       " 'shooting': 27.596130000000006}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wmd_pairwise_flow[2].items(), key=lambda item: item[1], reverse=True)[:30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Many-to-many WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a first attempt to do the flows from words between many documents, without first filtering using Gale-Shapeley. However, this proved too inefficient. As you can see looking at the CPU times, it is very slow even with extremely small samples and the time complexity is quadratic (or worse?), meaning it rapidly gets even worse as the sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 1.92 s, total: 2min 18s\n",
      "Wall time: 54.3 s\n"
     ]
    }
   ],
   "source": [
    "%time m2m_distances = WMDManyToMany(nfl_docs[:20], nba_docs[:20],E,idx2word).get_distances(return_flow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 1.9 s, total: 2min 24s\n",
      "Wall time: 55.2 s\n"
     ]
    }
   ],
   "source": [
    "%time m2m_distances_flow, wc_X1, wc_X2 = WMDManyToMany(nfl_docs[:20],nba_docs[:20],E,idx2word).get_distances(return_flow = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'karen': 8.69223,\n",
       " 'wrenching': 8.31882,\n",
       " 'carpenter': 7.468960000000001,\n",
       " 'laughter': 7.467879999999999,\n",
       " 'liked': 6.864090000000003,\n",
       " 'mom': 6.791519999999999,\n",
       " 'gut': 6.759419999999999,\n",
       " 'love': 6.551409999999997,\n",
       " 'camp': 6.533080000000001,\n",
       " 'hr': 6.1393699999999995}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wc_X1.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hopper': 8.372459999999998,\n",
       " 'jake': 7.63837,\n",
       " 'movie': 7.267059999999995,\n",
       " 'film': 6.936379999999998,\n",
       " 'shakespeare': 5.99276,\n",
       " 'oddness': 5.53033,\n",
       " 'terrible': 4.943440000000001,\n",
       " 'parent': 4.751790000000001,\n",
       " 'actor': 4.672620000000001,\n",
       " 'bad': 4.430020000000002}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wc_X2.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

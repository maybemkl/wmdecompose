{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_dir = os.path.join(os.getcwd(), '..')\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikaelbrunila/Documents/Code/flow-wmd/venv-fwmd/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from flow_wmd.documents import Document\n",
    "from flow_wmd.models import LC_RWMD, WMD, WMDManyToMany\n",
    "from gensim.models import KeyedVectors\n",
    "from itertools import islice\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "imdb_data = pd.read_csv(f\"{PATH}IMDB_Dataset.csv\")\n",
    "stopword_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize cleanup functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to remove noisy formatting, lemmatizing and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing functions\n",
    "# Partly self-authored, partly from https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square <brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = re.sub('<br / ><br / >', ' ', text)\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Lemmatizing the text\n",
    "def simple_lemmatizer(text):\n",
    "    lemmatizer=WordNetLemmatizer() \n",
    "    text= ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, stopword_list, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove special formatting and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords before denoising, lemmatizing and removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "tokenizer=ToktokTokenizer()\n",
    "imdb_data['review_clean']= [remove_stopwords(r, stopword_list) for r in imdb_data['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoise, remove special characters, lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "imdb_data['review_clean']=imdb_data['review_clean'].apply(denoise_text)\n",
    "imdb_data['review_clean']=imdb_data['review_clean'].apply(remove_special_characters)\n",
    "imdb_data['review_clean']=imdb_data['review_clean'].apply(simple_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords again, after other preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "imdb_data['review_clean']= [remove_stopwords(r, stopword_list) for r in imdb_data['review_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data _before_ preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data _after_ preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewer mentioned watching oz episode hooked right exactly happened first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home many aryan muslim gangsta latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away would say main appeal show due fact go show dare forget pretty picture painted mainstream audience forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['review_clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Separate pos and neg reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = imdb_data[imdb_data.sentiment == \"positive\"].reset_index(drop=True)\n",
    "neg = imdb_data[imdb_data.sentiment == \"negative\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos.review_clean.tolist()\n",
    "neg = neg.review_clean.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenize and \"sample\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "pos_tok = list(map(tokenize, pos[:500]))\n",
    "neg_tok = list(map(tokenize, neg[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample = [\" \".join(doc) for doc in pos_tok]\n",
    "neg_sample = [\" \".join(doc) for doc in neg_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load pretrained Google News W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n",
      "CPU times: user 47.8 s, sys: 4.21 s, total: 52.1 s\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "def read_1w_corpus(name, sep=\"\\t\"):\n",
    "    for line in open(name):\n",
    "        yield line.split(sep)\n",
    "\n",
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load corpus and remove OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99 µs, sys: 820 µs, total: 919 µs\n",
      "Wall time: 1.51 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikaelbrunila/Documents/Code/flow-wmd/venv-fwmd/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 363 ms, sys: 1.06 s, total: 1.42 s\n",
      "Wall time: 2.87 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(norm='l1', tokenizer=<function tokenize at 0x7fadd27fb280>,\n",
       "                use_idf=False)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pos_sample + neg_sample\n",
    "\n",
    "%time vectorizer = TfidfVectorizer(use_idf=False, tokenizer=tokenize, norm='l1')\n",
    "%time vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 ms, sys: 5.47 ms, total: 30.3 ms\n",
      "Wall time: 35.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time oov = [word for word in vectorizer.get_feature_names() if word not in model.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3066"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.49 s, sys: 23.2 ms, total: 2.51 s\n",
      "Wall time: 2.54 s\n",
      "CPU times: user 2.33 s, sys: 13.4 ms, total: 2.34 s\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "#removing the oov words\n",
    "def remove_oov(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in oov]\n",
    "    #filtered_tokens = filter(lambda token: token not in oov, tokens)\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "%time pos_sample = list(map(remove_oov, pos_sample))\n",
    "%time neg_sample = list(map(remove_oov, neg_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewer mentioned watching oz episode hooked right exactly happened first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word called oz nickname given maximum security state focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home many aryan muslim gangsta latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away would say main appeal show due fact go show dare forget pretty picture painted mainstream audience forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 µs, sys: 1 µs, total: 30 µs\n",
      "Wall time: 31.9 µs\n",
      "CPU times: user 223 ms, sys: 9.34 ms, total: 232 ms\n",
      "Wall time: 242 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(norm='l1', tokenizer=<function tokenize at 0x7fadd27fb280>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pos_sample + neg_sample\n",
    "\n",
    "%time vectorizer = TfidfVectorizer(use_idf=True, tokenizer=tokenize,norm='l1')\n",
    "%time vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "pos_nbow = vectorizer.transform(pos_sample)\n",
    "neg_nbow = vectorizer.transform(neg_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tok = list(map(tokenize, pos_sample))\n",
    "neg_tok =  list(map(tokenize, neg_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewer',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scene',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tok[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 654 µs, total: 20.8 ms\n",
      "Wall time: 20.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time oov_ = [word for word in vectorizer.get_feature_names() if word not in model.key_to_index.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Get features and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "idx2word = {idx: word for idx, word in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the embedding matrix \"E\" for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.vstack([model.get_vector(word) for word in vectorizer.get_feature_names()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=100)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLUSTERS = 100\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS, verbose=0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2cluster = {features[idx]: cl for idx, cl in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('`', 60), ('aaargh', 60), ('aamir', 60), ('aaron', 81), ('ab', 1), ('abandon', 10), ('abandoned', 0), ('abba', 60), ('abbey', 68), ('abbot', 18)]\n"
     ]
    }
   ],
   "source": [
    "print(take(10, word2cluster.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2words = defaultdict(list)\n",
    "for key, value in word2cluster.items():\n",
    "    cluster2words[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abused', 'accosted', 'alive', 'amputated', 'asleep', 'assassinated', 'attacked', 'avenge', 'beaten', 'beheading', 'belonging', 'bitten', 'blinded', 'blindfolded', 'bloodstained', 'bombed', 'brutally', 'buried', 'burned', 'burnt', 'butchered', 'castrated', 'chained', 'chased', 'confronted', 'crushed', 'cursed', 'dead', 'deposed', 'desecrated', 'die', 'died', 'disarmed', 'disfigured', 'drowned', 'dumped', 'fatally', 'fired', 'fled', 'flee', 'fleeing', 'flogged', 'foiled', 'freed', 'grazed', 'gunned', 'gunning', 'hacked', 'hanged', 'harassed', 'harassing', 'hazed', 'hijacked', 'holed', 'hospitalized', 'hostage', 'hunted', 'infected', 'injured', 'kidnapped', 'kill', 'killed', 'killing', 'lynch', 'manhandled', 'menaced', 'misbehaving', 'missing', 'murdered', 'mutilate', 'mutilated', 'overthrown', 'poisoned', 'punished', 'raided', 'rampaged', 'raped', 'repelled', 'rescued', 'revenge', 'rob', 'robbed', 'semiconscious', 'severed', 'slain', 'slept', 'slit', 'stabbed', 'starving', 'stole', 'stolen', 'stoned', 'stranded', 'strangled', 'strangling', 'taunted', 'terrorized', 'tortured', 'torturing']\n"
     ]
    }
   ],
   "source": [
    "print(cluster2words[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Initialize documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all reviews into \"documents\", each with a set of weights per word in the corpus (\"nbow\"), the sum of these weights (\"weights_sum\"), the indeces of the words in the documents (\"idxs\") and the word vectors corresponding to each word (\"vecs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "pos_docs, neg_docs = [], []\n",
    "\n",
    "for idx, doc in enumerate(pos_tok):\n",
    "    pos_docs.append(Document(doc, pos_nbow[idx], word2idx, E))\n",
    "    \n",
    "for idx, doc in enumerate(neg_tok):\n",
    "    neg_docs.append(Document(doc, neg_nbow[idx], word2idx, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_docs[0].nbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_docs[0].weights_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12299, 9740, 1036, 2071, 10266, 5148, 13342, 13344, 11302, 5673]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_docs[0].idxs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16992188,  0.04907227,  0.08154297,  0.12011719, -0.14746094,\n",
       "        0.0291748 ,  0.36523438, -0.10107422,  0.125     ,  0.04516602],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_docs[0].vecs[:1][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Linear-Complexity Relaxed WMD (LC-RWMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the [Linear-Complexity Relaxed WMD](https://arxiv.org/abs/1711.07227) to get the distances between all positive and all negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 606 µs, sys: 1.03 ms, total: 1.63 ms\n",
      "Wall time: 4.21 ms\n",
      "CPU times: user 2min 15s, sys: 22.3 s, total: 2min 37s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%time lc_rwmd = LC_RWMD(pos_docs, neg_docs,pos_nbow,neg_nbow,E)\n",
    "%time lc_rwmd.get_D()\n",
    "#%time lc_rwmd.get_L(1)\n",
    "#%time lc_rwmd.get_rwmd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Gale-Shapeley Pairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Gale-Shapeley matching algorithm](https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm) to find the optimal pairs between positive and negative reviews. This iterates over all the reviews and finds the set of matches that pairs each review with its optimal match given that all positive reviews have to be matched with a negative review and vice versa. The output is a dictionary of key-value pairs, where each pair represents an optimal match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow_wmd.gale_shapeley import Matcher\n",
    "\n",
    "matcher = Matcher(lc_rwmd.D)\n",
    "engaged = matcher.matchmaker()\n",
    "matcher.check()\n",
    "pairs = engaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output of Gale-Shapeley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(137, 497),\n",
       " (388, 468),\n",
       " (95, 360),\n",
       " (454, 361),\n",
       " (259, 363),\n",
       " (263, 146),\n",
       " (143, 90),\n",
       " (224, 107),\n",
       " (91, 108),\n",
       " (443, 347)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, pairs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Pairwise WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pairwise distances between the documents selected by the Galey-Shapeley algorithm _without_ returning the flow between individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances between 0 documents.\n",
      "Calculated distances between 100 documents.\n",
      "Calculated distances between 200 documents.\n",
      "Calculated distances between 300 documents.\n",
      "Calculated distances between 400 documents.\n",
      "CPU times: user 4min 10s, sys: 16.1 s, total: 4min 26s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "from flow_wmd.models import WMDPairs\n",
    "\n",
    "wmd_pairs = WMDPairs(pos_docs,neg_docs,pairs,E,idx2word)\n",
    "%time wmd_pairs.get_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value is a matrix of distances between the document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairs.distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the pairwise distances between the documents selected by the Galey-Shapeley algorithm, this time also returning the flow between individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated distances between 0 documents.\n",
      "Calculated distances between 100 documents.\n",
      "Calculated distances between 200 documents.\n",
      "Calculated distances between 300 documents.\n",
      "Calculated distances between 400 documents.\n",
      "CPU times: user 4min 40s, sys: 17.2 s, total: 4min 57s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "wmd_pairs_flow = WMDPairs(pos_docs,neg_docs,pairs,E,idx2word)\n",
    "%time wmd_pairs_flow.get_distances(return_flow = True, sum_clusters = True, w2c = word2cluster, c2w = cluster2words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three return values.\n",
    "\n",
    "The first one is again a matrix of distances between the document pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairs_flow.distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second return value is a list of tuples with all the words that contributed the most to the distance from the positive documents to the negative ones. These are _not_ sorted from high to low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exploration', 0.049010000000000005),\n",
       " ('double', 0.18775),\n",
       " ('delinquency', 0.20461),\n",
       " ('distortion', 0.11278),\n",
       " ('pet', 0.24564),\n",
       " ('granddaughter', 0.1211),\n",
       " ('hubris', 0.03926),\n",
       " ('hardened', 0.04453),\n",
       " ('sleeker', 0.03),\n",
       " ('pu', 0.02374)]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairs_flow.wc_X1.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third return value is a list of tuples with all the words that contributed the most to the distance from the negative documents to the positive ones. Again, these are _not_ sorted from high to low or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('double', 0.12838),\n",
       " ('pet', 0.16274000000000002),\n",
       " ('hardened', 0.07715),\n",
       " ('lurch', 0.0698),\n",
       " ('underwear', 0.02577),\n",
       " ('corpse', 0.026479999999999997),\n",
       " ('primeval', 0.043500000000000004),\n",
       " ('threesome', 0.10307999999999999),\n",
       " ('skate', 0.15538),\n",
       " ('ritualistic', 0.06027)]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairs_flow.wc_X2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 23.696899999999985),\n",
       " (81, 39.47520000000002),\n",
       " (1, 16.666900000000005),\n",
       " (10, 37.608569999999965),\n",
       " (0, 7.430600000000002),\n",
       " (68, 6.140189999999999),\n",
       " (18, 8.9413),\n",
       " (90, 21.92162),\n",
       " (92, 8.460380000000004),\n",
       " (19, 6.029949999999999)]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairs_flow.cc_X1.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, 17.831660000000003),\n",
       " (81, 22.719140000000017),\n",
       " (1, 15.736809999999993),\n",
       " (10, 35.36103999999994),\n",
       " (0, 11.203750000000003),\n",
       " (68, 3.5156499999999995),\n",
       " (18, 10.151929999999995),\n",
       " (90, 24.523450000000004),\n",
       " (92, 10.510490000000006),\n",
       " (19, 7.159270000000002)]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take(10, wmd_pairs_flow.cc_X2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{36: 89.03332999999994,\n",
       " 62: 84.44415999999983,\n",
       " 48: 44.972770000000025,\n",
       " 81: 39.47520000000002,\n",
       " 10: 37.608569999999965,\n",
       " 27: 32.46946,\n",
       " 52: 32.33321000000001,\n",
       " 58: 32.24520000000002,\n",
       " 75: 31.084949999999996,\n",
       " 25: 30.576999999999995}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wmd_pairs_flow.cc_X1.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Intepreting pairwise WMD flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sort the distances of the words that created the most distance from the positive to the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'film': 4.534129999999999,\n",
       " 'movie': 3.938469999999998,\n",
       " 'story': 3.434129999999999,\n",
       " 'great': 3.077940000000002,\n",
       " 'love': 2.9721399999999996,\n",
       " 'performance': 2.73945,\n",
       " 'life': 2.426700000000001,\n",
       " 'character': 2.39385,\n",
       " 'best': 2.340230000000001,\n",
       " 'well': 2.3356999999999988,\n",
       " 'watch': 2.2191299999999994,\n",
       " 'show': 2.1959499999999994,\n",
       " 'still': 2.195579999999999,\n",
       " 'scene': 2.19522,\n",
       " 'good': 2.1382,\n",
       " 'time': 2.09075,\n",
       " 'comedy': 2.08384,\n",
       " 'young': 2.0808300000000015,\n",
       " 'one': 2.0784700000000003,\n",
       " 'like': 2.015480000000001,\n",
       " 'dvd': 2.01107,\n",
       " 'excellent': 2.0090300000000005,\n",
       " 'people': 1.9920699999999996,\n",
       " 'fan': 1.9595799999999997,\n",
       " 'actor': 1.87092,\n",
       " 'role': 1.86876,\n",
       " 'see': 1.8347,\n",
       " 'little': 1.8331200000000005,\n",
       " 'loved': 1.83208,\n",
       " 'family': 1.7825199999999999}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wmd_pairs_flow.wc_X1.items(), key=lambda item: item[1], reverse=True)[:30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what added most distance when moving from the negative to the positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': 6.2731,\n",
       " 'bad': 5.4052500000000006,\n",
       " 'film': 4.1417399999999995,\n",
       " 'worst': 3.635940000000001,\n",
       " 'plot': 3.55549,\n",
       " 'waste': 3.1293699999999993,\n",
       " 'scene': 3.013090000000001,\n",
       " 'acting': 2.7281000000000004,\n",
       " 'character': 2.5816599999999994,\n",
       " 'made': 2.4689400000000004,\n",
       " 'rating': 2.45743,\n",
       " 'like': 2.45132,\n",
       " 'ever': 2.3756300000000006,\n",
       " 'stupid': 2.34381,\n",
       " 'actor': 2.2481300000000006,\n",
       " 'story': 2.22648,\n",
       " 'funny': 2.2264,\n",
       " 'really': 2.2117,\n",
       " 'boring': 2.2083199999999996,\n",
       " 'watch': 2.19631,\n",
       " 'good': 2.1373299999999995,\n",
       " 'even': 2.0962,\n",
       " 'thing': 2.0919399999999997,\n",
       " 'money': 2.0855600000000005,\n",
       " 'watching': 2.039080000000001,\n",
       " 'guy': 1.9936299999999998,\n",
       " 'awful': 1.9861900000000003,\n",
       " 'would': 1.9547700000000006,\n",
       " 'seen': 1.9501299999999995,\n",
       " 'see': 1.93109}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wmd_pairs_flow.wc_X2.items(), key=lambda item: item[1], reverse=True)[:30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(60, 17.831660000000003), (81, 22.719140000000017), (1, 15.736809999999993), (10, 35.36103999999994), (0, 11.203750000000003), (68, 3.5156499999999995), (18, 10.151929999999995), (90, 24.523450000000004), (92, 10.510490000000006), (19, 7.159270000000002), (70, 19.557169999999992), (96, 1.8248), (34, 32.90991000000001), (47, 21.336429999999993), (40, 1.1524), (52, 22.593739999999997), (74, 10.246), (75, 24.458830000000013), (73, 5.03237), (48, 46.176009999999955), (3, 12.960470000000008), (62, 77.90586000000003), (43, 13.134669999999987), (4, 14.312549999999991), (36, 102.26358999999995), (87, 23.00473000000001), (32, 4.854150000000001), (56, 7.2088199999999985), (33, 23.04513), (54, 9.73582000000001), (61, 7.750219999999999), (24, 20.703079999999993), (9, 13.617820000000012), (80, 8.937149999999999), (12, 8.760590000000002), (23, 35.758649999999996), (85, 24.82207), (5, 8.449319999999998), (57, 21.36416), (6, 11.796859999999995), (8, 12.94871000000001), (31, 9.361969999999998), (95, 5.326039999999999), (58, 28.104680000000005), (46, 15.789740000000009), (15, 20.520319999999995), (84, 9.05655), (64, 12.98305), (30, 13.189389999999996), (16, 6.525079999999999), (39, 13.792269999999991), (42, 6.189449999999999), (94, 14.758360000000007), (26, 28.794969999999992), (78, 12.575340000000002), (37, 22.860979999999973), (38, 15.117090000000003), (2, 14.743290000000005), (27, 11.278370000000008), (14, 9.281290000000002), (17, 4.0318), (66, 6.68789), (89, 8.424089999999996), (28, 6.572449999999997), (99, 8.186099999999998), (22, 18.135529999999996), (45, 4.12558), (59, 4.457079999999999), (35, 5.2400400000000005), (77, 7.749099999999999), (91, 2.7352099999999995), (25, 21.31816), (82, 18.447210000000002), (83, 2.324960000000001), (13, 11.74226), (7, 16.004080000000002), (65, 27.367709999999978), (51, 11.671929999999993), (11, 10.253029999999997), (76, 12.398300000000003), (55, 4.34879), (63, 16.050439999999995), (93, 3.4742999999999995), (44, 4.59088), (20, 17.211789999999997), (97, 7.499560000000001), (21, 5.13559), (53, 5.128269999999999), (41, 31.37304999999999), (88, 1.2480300000000002), (86, 29.42222999999996), (29, 7.511819999999998), (49, 7.492510000000002), (98, 13.50760999999999), (72, 5.36535), (71, 6.46163), (79, 19.567609999999995), (69, 2.16582), (50, 0.35003000000000006), (67, 0.11268)])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmd_pairs_flow.cc_X2.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60, 23.696899999999985), (81, 39.47520000000002), (1, 16.666900000000005), (10, 37.608569999999965), (0, 7.430600000000002), (68, 6.140189999999999), (18, 8.9413), (90, 21.92162), (92, 8.460380000000004), (19, 6.029949999999999)]\n",
      "[(60, 17.831660000000003), (81, 22.719140000000017), (1, 15.736809999999993), (10, 35.36103999999994), (0, 11.203750000000003), (68, 3.5156499999999995), (18, 10.151929999999995), (90, 24.523450000000004), (92, 10.510490000000006), (19, 7.159270000000002)]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 50\n",
    "n_words = 10\n",
    "\n",
    "def output_clusters(wc, cc, c2w, n_clusters, n_words):\n",
    "    top_clusters = [k for k, v in sorted(cc, key=lambda item: item[1], reverse=True)[:n_clusters]]\n",
    "    print(take(10, cc))\n",
    "    word_rankings = {k: v for k, v in sorted(wc, key=lambda item: item[1], reverse=True)}\n",
    "    keywords = []\n",
    "    for c in top_clusters:\n",
    "        cluster_words = {w: word_rankings[w] for w in c2w[c] if w in word_rankings.keys()}\n",
    "        top_c_words = [f\"{k} ({round(v, 2)})\" for k, v in sorted(cluster_words.items(), \n",
    "                                               key=lambda item: item[1], \n",
    "                                               reverse=True)[:n_words]]\n",
    "        keywords.append(top_c_words)\n",
    "    keywords_df = pd.DataFrame(keywords).transpose()\n",
    "    keywords_df.columns = top_clusters\n",
    "    return keywords_df\n",
    "\n",
    "c1 = output_clusters(wmd_pairs_flow.wc_X1.items(), wmd_pairs_flow.cc_X1.items(), cluster2words, n_clusters, n_words)\n",
    "c2 = output_clusters(wmd_pairs_flow.wc_X2.items(), wmd_pairs_flow.cc_X2.items(), cluster2words, n_clusters, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>36</th>\n",
       "      <th>62</th>\n",
       "      <th>48</th>\n",
       "      <th>81</th>\n",
       "      <th>10</th>\n",
       "      <th>27</th>\n",
       "      <th>52</th>\n",
       "      <th>58</th>\n",
       "      <th>75</th>\n",
       "      <th>25</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>65</th>\n",
       "      <th>11</th>\n",
       "      <th>9</th>\n",
       "      <th>61</th>\n",
       "      <th>63</th>\n",
       "      <th>39</th>\n",
       "      <th>32</th>\n",
       "      <th>49</th>\n",
       "      <th>43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well (2.34)</td>\n",
       "      <td>performance (2.74)</td>\n",
       "      <td>effect (1.15)</td>\n",
       "      <td>john (1.06)</td>\n",
       "      <td>make (1.57)</td>\n",
       "      <td>great (3.08)</td>\n",
       "      <td>u (1.35)</td>\n",
       "      <td>role (1.87)</td>\n",
       "      <td>man (1.67)</td>\n",
       "      <td>dvd (2.01)</td>\n",
       "      <td>...</td>\n",
       "      <td>fan (1.96)</td>\n",
       "      <td>mickey (0.68)</td>\n",
       "      <td>house (0.85)</td>\n",
       "      <td>black (1.0)</td>\n",
       "      <td>art (1.16)</td>\n",
       "      <td>evil (0.93)</td>\n",
       "      <td>heart (0.94)</td>\n",
       "      <td>romantic (1.19)</td>\n",
       "      <td>taste (0.54)</td>\n",
       "      <td>question (0.87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>still (2.2)</td>\n",
       "      <td>show (2.2)</td>\n",
       "      <td>human (1.03)</td>\n",
       "      <td>robert (0.91)</td>\n",
       "      <td>get (1.57)</td>\n",
       "      <td>excellent (2.01)</td>\n",
       "      <td>oscar (0.78)</td>\n",
       "      <td>work (1.72)</td>\n",
       "      <td>old (1.65)</td>\n",
       "      <td>tv (1.47)</td>\n",
       "      <td>...</td>\n",
       "      <td>actor (1.87)</td>\n",
       "      <td>guy (0.62)</td>\n",
       "      <td>seat (0.81)</td>\n",
       "      <td>white (0.75)</td>\n",
       "      <td>school (0.88)</td>\n",
       "      <td>mystery (0.76)</td>\n",
       "      <td>brain (0.67)</td>\n",
       "      <td>seductive (0.42)</td>\n",
       "      <td>meatball (0.48)</td>\n",
       "      <td>problem (0.66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like (2.02)</td>\n",
       "      <td>time (2.09)</td>\n",
       "      <td>high (0.86)</td>\n",
       "      <td>mary (0.89)</td>\n",
       "      <td>take (1.54)</td>\n",
       "      <td>wonderful (1.65)</td>\n",
       "      <td>cant (0.51)</td>\n",
       "      <td>part (1.38)</td>\n",
       "      <td>friend (1.31)</td>\n",
       "      <td>italian (1.32)</td>\n",
       "      <td>...</td>\n",
       "      <td>hero (0.87)</td>\n",
       "      <td>hell (0.56)</td>\n",
       "      <td>room (0.55)</td>\n",
       "      <td>brown (0.67)</td>\n",
       "      <td>lesson (0.58)</td>\n",
       "      <td>ghost (0.44)</td>\n",
       "      <td>memory (0.67)</td>\n",
       "      <td>artistic (0.4)</td>\n",
       "      <td>popcorn (0.32)</td>\n",
       "      <td>situation (0.66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people (1.99)</td>\n",
       "      <td>one (2.08)</td>\n",
       "      <td>number (0.84)</td>\n",
       "      <td>joe (0.84)</td>\n",
       "      <td>find (1.36)</td>\n",
       "      <td>entertaining (1.41)</td>\n",
       "      <td>batman (0.41)</td>\n",
       "      <td>relationship (0.98)</td>\n",
       "      <td>girl (1.22)</td>\n",
       "      <td>disney (1.19)</td>\n",
       "      <td>...</td>\n",
       "      <td>writer (0.68)</td>\n",
       "      <td>stuff (0.54)</td>\n",
       "      <td>street (0.51)</td>\n",
       "      <td>accent (0.59)</td>\n",
       "      <td>student (0.45)</td>\n",
       "      <td>universe (0.4)</td>\n",
       "      <td>gene (0.61)</td>\n",
       "      <td>imagery (0.36)</td>\n",
       "      <td>ham (0.3)</td>\n",
       "      <td>reason (0.56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>little (1.83)</td>\n",
       "      <td>year (1.57)</td>\n",
       "      <td>small (0.82)</td>\n",
       "      <td>tom (0.81)</td>\n",
       "      <td>go (1.35)</td>\n",
       "      <td>perfect (1.36)</td>\n",
       "      <td>betty (0.41)</td>\n",
       "      <td>theme (0.95)</td>\n",
       "      <td>actress (1.1)</td>\n",
       "      <td>american (1.18)</td>\n",
       "      <td>...</td>\n",
       "      <td>artist (0.44)</td>\n",
       "      <td>yes (0.5)</td>\n",
       "      <td>glass (0.5)</td>\n",
       "      <td>costume (0.52)</td>\n",
       "      <td>chemistry (0.44)</td>\n",
       "      <td>zodiac (0.36)</td>\n",
       "      <td>hospital (0.55)</td>\n",
       "      <td>subtle (0.33)</td>\n",
       "      <td>eat (0.26)</td>\n",
       "      <td>flaw (0.52)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>real (1.76)</td>\n",
       "      <td>cast (1.51)</td>\n",
       "      <td>usual (0.77)</td>\n",
       "      <td>james (0.77)</td>\n",
       "      <td>keep (1.3)</td>\n",
       "      <td>brilliant (1.27)</td>\n",
       "      <td>fx (0.41)</td>\n",
       "      <td>direction (0.8)</td>\n",
       "      <td>father (1.06)</td>\n",
       "      <td>hollywood (1.09)</td>\n",
       "      <td>...</td>\n",
       "      <td>guest (0.34)</td>\n",
       "      <td>crazy (0.47)</td>\n",
       "      <td>wheelchair (0.5)</td>\n",
       "      <td>jean (0.5)</td>\n",
       "      <td>academy (0.44)</td>\n",
       "      <td>planet (0.35)</td>\n",
       "      <td>disease (0.36)</td>\n",
       "      <td>visually (0.32)</td>\n",
       "      <td>fat (0.26)</td>\n",
       "      <td>issue (0.49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>much (1.72)</td>\n",
       "      <td>first (1.32)</td>\n",
       "      <td>major (0.76)</td>\n",
       "      <td>jim (0.77)</td>\n",
       "      <td>give (1.28)</td>\n",
       "      <td>nice (1.14)</td>\n",
       "      <td>fav (0.41)</td>\n",
       "      <td>experience (0.77)</td>\n",
       "      <td>woman (1.04)</td>\n",
       "      <td>york (0.7)</td>\n",
       "      <td>...</td>\n",
       "      <td>reviewer (0.33)</td>\n",
       "      <td>okay (0.39)</td>\n",
       "      <td>box (0.4)</td>\n",
       "      <td>hair (0.37)</td>\n",
       "      <td>class (0.44)</td>\n",
       "      <td>villain (0.32)</td>\n",
       "      <td>psychological (0.34)</td>\n",
       "      <td>mannerism (0.3)</td>\n",
       "      <td>cream (0.26)</td>\n",
       "      <td>challenge (0.39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>way (1.57)</td>\n",
       "      <td>special (1.22)</td>\n",
       "      <td>across (0.74)</td>\n",
       "      <td>george (0.76)</td>\n",
       "      <td>become (0.94)</td>\n",
       "      <td>amazing (1.08)</td>\n",
       "      <td>reese (0.35)</td>\n",
       "      <td>atmosphere (0.75)</td>\n",
       "      <td>brother (0.99)</td>\n",
       "      <td>australia (0.6)</td>\n",
       "      <td>...</td>\n",
       "      <td>leader (0.3)</td>\n",
       "      <td>momma (0.39)</td>\n",
       "      <td>bar (0.32)</td>\n",
       "      <td>makeup (0.34)</td>\n",
       "      <td>photography (0.4)</td>\n",
       "      <td>witch (0.31)</td>\n",
       "      <td>facial (0.27)</td>\n",
       "      <td>visual (0.29)</td>\n",
       "      <td>oz (0.25)</td>\n",
       "      <td>lack (0.35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>many (1.54)</td>\n",
       "      <td>day (1.18)</td>\n",
       "      <td>typical (0.7)</td>\n",
       "      <td>jones (0.75)</td>\n",
       "      <td>turn (0.88)</td>\n",
       "      <td>interesting (1.0)</td>\n",
       "      <td>pb (0.33)</td>\n",
       "      <td>depth (0.7)</td>\n",
       "      <td>kid (0.99)</td>\n",
       "      <td>america (0.58)</td>\n",
       "      <td>...</td>\n",
       "      <td>critic (0.29)</td>\n",
       "      <td>hey (0.32)</td>\n",
       "      <td>apartment (0.31)</td>\n",
       "      <td>blue (0.34)</td>\n",
       "      <td>computer (0.35)</td>\n",
       "      <td>creepy (0.3)</td>\n",
       "      <td>blood (0.25)</td>\n",
       "      <td>wistful (0.29)</td>\n",
       "      <td>eating (0.25)</td>\n",
       "      <td>difficulty (0.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>really (1.5)</td>\n",
       "      <td>short (1.15)</td>\n",
       "      <td>rating (0.69)</td>\n",
       "      <td>mr (0.74)</td>\n",
       "      <td>meet (0.87)</td>\n",
       "      <td>awesome (0.94)</td>\n",
       "      <td>victoria (0.32)</td>\n",
       "      <td>future (0.66)</td>\n",
       "      <td>mother (0.89)</td>\n",
       "      <td>japan (0.58)</td>\n",
       "      <td>...</td>\n",
       "      <td>performer (0.24)</td>\n",
       "      <td>oh (0.29)</td>\n",
       "      <td>hall (0.26)</td>\n",
       "      <td>color (0.24)</td>\n",
       "      <td>college (0.34)</td>\n",
       "      <td>pirate (0.29)</td>\n",
       "      <td>ill (0.24)</td>\n",
       "      <td>symbolism (0.23)</td>\n",
       "      <td>tucker (0.21)</td>\n",
       "      <td>difference (0.29)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              36                  62             48             81  \\\n",
       "0    well (2.34)  performance (2.74)  effect (1.15)    john (1.06)   \n",
       "1    still (2.2)          show (2.2)   human (1.03)  robert (0.91)   \n",
       "2    like (2.02)         time (2.09)    high (0.86)    mary (0.89)   \n",
       "3  people (1.99)          one (2.08)  number (0.84)     joe (0.84)   \n",
       "4  little (1.83)         year (1.57)   small (0.82)     tom (0.81)   \n",
       "5    real (1.76)         cast (1.51)   usual (0.77)   james (0.77)   \n",
       "6    much (1.72)        first (1.32)   major (0.76)     jim (0.77)   \n",
       "7     way (1.57)      special (1.22)  across (0.74)  george (0.76)   \n",
       "8    many (1.54)          day (1.18)  typical (0.7)   jones (0.75)   \n",
       "9   really (1.5)        short (1.15)  rating (0.69)      mr (0.74)   \n",
       "\n",
       "              10                   27               52                   58  \\\n",
       "0    make (1.57)         great (3.08)         u (1.35)          role (1.87)   \n",
       "1     get (1.57)     excellent (2.01)     oscar (0.78)          work (1.72)   \n",
       "2    take (1.54)     wonderful (1.65)      cant (0.51)          part (1.38)   \n",
       "3    find (1.36)  entertaining (1.41)    batman (0.41)  relationship (0.98)   \n",
       "4      go (1.35)       perfect (1.36)     betty (0.41)         theme (0.95)   \n",
       "5     keep (1.3)     brilliant (1.27)        fx (0.41)      direction (0.8)   \n",
       "6    give (1.28)          nice (1.14)       fav (0.41)    experience (0.77)   \n",
       "7  become (0.94)       amazing (1.08)     reese (0.35)    atmosphere (0.75)   \n",
       "8    turn (0.88)    interesting (1.0)        pb (0.33)          depth (0.7)   \n",
       "9    meet (0.87)       awesome (0.94)  victoria (0.32)        future (0.66)   \n",
       "\n",
       "               75                25  ...                78             65  \\\n",
       "0      man (1.67)        dvd (2.01)  ...        fan (1.96)  mickey (0.68)   \n",
       "1      old (1.65)         tv (1.47)  ...      actor (1.87)     guy (0.62)   \n",
       "2   friend (1.31)    italian (1.32)  ...       hero (0.87)    hell (0.56)   \n",
       "3     girl (1.22)     disney (1.19)  ...     writer (0.68)   stuff (0.54)   \n",
       "4   actress (1.1)   american (1.18)  ...     artist (0.44)      yes (0.5)   \n",
       "5   father (1.06)  hollywood (1.09)  ...      guest (0.34)   crazy (0.47)   \n",
       "6    woman (1.04)        york (0.7)  ...   reviewer (0.33)    okay (0.39)   \n",
       "7  brother (0.99)   australia (0.6)  ...      leader (0.3)   momma (0.39)   \n",
       "8      kid (0.99)    america (0.58)  ...     critic (0.29)     hey (0.32)   \n",
       "9   mother (0.89)      japan (0.58)  ...  performer (0.24)      oh (0.29)   \n",
       "\n",
       "                 11              9                  61              63  \\\n",
       "0      house (0.85)     black (1.0)         art (1.16)     evil (0.93)   \n",
       "1       seat (0.81)    white (0.75)      school (0.88)  mystery (0.76)   \n",
       "2       room (0.55)    brown (0.67)      lesson (0.58)    ghost (0.44)   \n",
       "3     street (0.51)   accent (0.59)     student (0.45)  universe (0.4)   \n",
       "4       glass (0.5)  costume (0.52)   chemistry (0.44)   zodiac (0.36)   \n",
       "5  wheelchair (0.5)      jean (0.5)     academy (0.44)   planet (0.35)   \n",
       "6         box (0.4)     hair (0.37)       class (0.44)  villain (0.32)   \n",
       "7        bar (0.32)   makeup (0.34)  photography (0.4)    witch (0.31)   \n",
       "8  apartment (0.31)     blue (0.34)    computer (0.35)    creepy (0.3)   \n",
       "9       hall (0.26)    color (0.24)     college (0.34)   pirate (0.29)   \n",
       "\n",
       "                     39                32               49                 43  \n",
       "0          heart (0.94)   romantic (1.19)     taste (0.54)    question (0.87)  \n",
       "1          brain (0.67)  seductive (0.42)  meatball (0.48)     problem (0.66)  \n",
       "2         memory (0.67)    artistic (0.4)   popcorn (0.32)   situation (0.66)  \n",
       "3           gene (0.61)    imagery (0.36)        ham (0.3)      reason (0.56)  \n",
       "4       hospital (0.55)     subtle (0.33)       eat (0.26)        flaw (0.52)  \n",
       "5        disease (0.36)   visually (0.32)       fat (0.26)       issue (0.49)  \n",
       "6  psychological (0.34)   mannerism (0.3)     cream (0.26)   challenge (0.39)  \n",
       "7         facial (0.27)     visual (0.29)        oz (0.25)        lack (0.35)  \n",
       "8          blood (0.25)    wistful (0.29)    eating (0.25)   difficulty (0.3)  \n",
       "9            ill (0.24)  symbolism (0.23)    tucker (0.21)  difference (0.29)  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>36</th>\n",
       "      <th>62</th>\n",
       "      <th>48</th>\n",
       "      <th>23</th>\n",
       "      <th>10</th>\n",
       "      <th>34</th>\n",
       "      <th>41</th>\n",
       "      <th>86</th>\n",
       "      <th>26</th>\n",
       "      <th>58</th>\n",
       "      <th>...</th>\n",
       "      <th>98</th>\n",
       "      <th>30</th>\n",
       "      <th>43</th>\n",
       "      <th>64</th>\n",
       "      <th>3</th>\n",
       "      <th>8</th>\n",
       "      <th>78</th>\n",
       "      <th>76</th>\n",
       "      <th>6</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like (2.45)</td>\n",
       "      <td>one (1.93)</td>\n",
       "      <td>rating (2.46)</td>\n",
       "      <td>watch (2.2)</td>\n",
       "      <td>get (1.88)</td>\n",
       "      <td>would (1.95)</td>\n",
       "      <td>movie (6.27)</td>\n",
       "      <td>got (1.58)</td>\n",
       "      <td>action (1.78)</td>\n",
       "      <td>idea (1.34)</td>\n",
       "      <td>...</td>\n",
       "      <td>pretty (1.61)</td>\n",
       "      <td>bored (1.11)</td>\n",
       "      <td>lack (1.29)</td>\n",
       "      <td>sense (1.32)</td>\n",
       "      <td>life (1.39)</td>\n",
       "      <td>name (1.28)</td>\n",
       "      <td>actor (2.25)</td>\n",
       "      <td>animal (0.88)</td>\n",
       "      <td>poorly (1.12)</td>\n",
       "      <td>worse (1.22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever (2.38)</td>\n",
       "      <td>show (1.92)</td>\n",
       "      <td>effect (1.3)</td>\n",
       "      <td>see (1.93)</td>\n",
       "      <td>make (1.87)</td>\n",
       "      <td>could (1.78)</td>\n",
       "      <td>film (4.14)</td>\n",
       "      <td>saw (1.09)</td>\n",
       "      <td>comment (1.32)</td>\n",
       "      <td>work (1.24)</td>\n",
       "      <td>...</td>\n",
       "      <td>totally (1.06)</td>\n",
       "      <td>sorry (0.87)</td>\n",
       "      <td>problem (1.21)</td>\n",
       "      <td>talent (1.14)</td>\n",
       "      <td>child (1.23)</td>\n",
       "      <td>star (1.07)</td>\n",
       "      <td>fan (1.25)</td>\n",
       "      <td>creature (0.79)</td>\n",
       "      <td>badly (0.75)</td>\n",
       "      <td>slow (0.87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>really (2.21)</td>\n",
       "      <td>time (1.74)</td>\n",
       "      <td>main (0.87)</td>\n",
       "      <td>say (1.86)</td>\n",
       "      <td>go (1.76)</td>\n",
       "      <td>try (1.28)</td>\n",
       "      <td>scene (3.01)</td>\n",
       "      <td>seemed (1.06)</td>\n",
       "      <td>case (1.29)</td>\n",
       "      <td>part (1.09)</td>\n",
       "      <td>...</td>\n",
       "      <td>completely (0.95)</td>\n",
       "      <td>tired (0.79)</td>\n",
       "      <td>reason (1.17)</td>\n",
       "      <td>charm (0.72)</td>\n",
       "      <td>care (0.9)</td>\n",
       "      <td>world (0.8)</td>\n",
       "      <td>writer (0.98)</td>\n",
       "      <td>fox (0.65)</td>\n",
       "      <td>relentlessly (0.69)</td>\n",
       "      <td>serious (0.77)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>even (2.1)</td>\n",
       "      <td>cast (1.62)</td>\n",
       "      <td>low (0.86)</td>\n",
       "      <td>look (1.64)</td>\n",
       "      <td>find (1.36)</td>\n",
       "      <td>want (1.26)</td>\n",
       "      <td>viewer (1.15)</td>\n",
       "      <td>played (0.96)</td>\n",
       "      <td>review (1.21)</td>\n",
       "      <td>direction (0.9)</td>\n",
       "      <td>...</td>\n",
       "      <td>quite (0.82)</td>\n",
       "      <td>confused (0.62)</td>\n",
       "      <td>mess (0.73)</td>\n",
       "      <td>imagination (0.43)</td>\n",
       "      <td>family (0.79)</td>\n",
       "      <td>blockbuster (0.58)</td>\n",
       "      <td>producer (0.79)</td>\n",
       "      <td>chick (0.56)</td>\n",
       "      <td>quickly (0.61)</td>\n",
       "      <td>hot (0.66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thing (2.09)</td>\n",
       "      <td>first (1.59)</td>\n",
       "      <td>overall (0.84)</td>\n",
       "      <td>recommend (1.15)</td>\n",
       "      <td>save (1.17)</td>\n",
       "      <td>enough (1.26)</td>\n",
       "      <td>gore (1.1)</td>\n",
       "      <td>gave (0.95)</td>\n",
       "      <td>dialogue (1.16)</td>\n",
       "      <td>premise (0.88)</td>\n",
       "      <td>...</td>\n",
       "      <td>truly (0.82)</td>\n",
       "      <td>hated (0.6)</td>\n",
       "      <td>disappointment (0.56)</td>\n",
       "      <td>emotion (0.29)</td>\n",
       "      <td>live (0.49)</td>\n",
       "      <td>dream (0.42)</td>\n",
       "      <td>hero (0.72)</td>\n",
       "      <td>rat (0.46)</td>\n",
       "      <td>often (0.35)</td>\n",
       "      <td>tedious (0.58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>much (1.77)</td>\n",
       "      <td>end (1.57)</td>\n",
       "      <td>standard (0.66)</td>\n",
       "      <td>know (1.14)</td>\n",
       "      <td>give (1.12)</td>\n",
       "      <td>must (1.16)</td>\n",
       "      <td>flick (1.06)</td>\n",
       "      <td>went (0.93)</td>\n",
       "      <td>budget (0.99)</td>\n",
       "      <td>role (0.84)</td>\n",
       "      <td>...</td>\n",
       "      <td>especially (0.46)</td>\n",
       "      <td>sick (0.57)</td>\n",
       "      <td>situation (0.49)</td>\n",
       "      <td>oddness (0.27)</td>\n",
       "      <td>kibbutz (0.45)</td>\n",
       "      <td>hype (0.36)</td>\n",
       "      <td>critic (0.56)</td>\n",
       "      <td>spider (0.4)</td>\n",
       "      <td>self (0.28)</td>\n",
       "      <td>confusing (0.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>people (1.74)</td>\n",
       "      <td>least (1.5)</td>\n",
       "      <td>single (0.66)</td>\n",
       "      <td>mean (1.05)</td>\n",
       "      <td>take (0.97)</td>\n",
       "      <td>please (1.11)</td>\n",
       "      <td>screen (0.91)</td>\n",
       "      <td>watched (0.92)</td>\n",
       "      <td>opinion (0.69)</td>\n",
       "      <td>example (0.71)</td>\n",
       "      <td>...</td>\n",
       "      <td>extremely (0.46)</td>\n",
       "      <td>spoiled (0.43)</td>\n",
       "      <td>trouble (0.46)</td>\n",
       "      <td>sheer (0.27)</td>\n",
       "      <td>native (0.37)</td>\n",
       "      <td>former (0.36)</td>\n",
       "      <td>godfather (0.39)</td>\n",
       "      <td>hunter (0.38)</td>\n",
       "      <td>ambiguously (0.23)</td>\n",
       "      <td>impossible (0.45)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nothing (1.62)</td>\n",
       "      <td>hour (1.48)</td>\n",
       "      <td>along (0.64)</td>\n",
       "      <td>understand (1.0)</td>\n",
       "      <td>turn (0.91)</td>\n",
       "      <td>trying (1.06)</td>\n",
       "      <td>imdb (0.86)</td>\n",
       "      <td>left (0.86)</td>\n",
       "      <td>group (0.67)</td>\n",
       "      <td>effort (0.65)</td>\n",
       "      <td>...</td>\n",
       "      <td>highly (0.38)</td>\n",
       "      <td>angry (0.42)</td>\n",
       "      <td>difference (0.42)</td>\n",
       "      <td>spirit (0.27)</td>\n",
       "      <td>country (0.36)</td>\n",
       "      <td>attention (0.34)</td>\n",
       "      <td>reviewer (0.37)</td>\n",
       "      <td>joey (0.37)</td>\n",
       "      <td>briefly (0.22)</td>\n",
       "      <td>extreme (0.39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>think (1.59)</td>\n",
       "      <td>back (1.3)</td>\n",
       "      <td>light (0.62)</td>\n",
       "      <td>sit (0.99)</td>\n",
       "      <td>keep (0.78)</td>\n",
       "      <td>need (1.02)</td>\n",
       "      <td>sequel (0.81)</td>\n",
       "      <td>lost (0.84)</td>\n",
       "      <td>act (0.66)</td>\n",
       "      <td>view (0.61)</td>\n",
       "      <td>...</td>\n",
       "      <td>generally (0.33)</td>\n",
       "      <td>scared (0.41)</td>\n",
       "      <td>question (0.39)</td>\n",
       "      <td>ego (0.23)</td>\n",
       "      <td>living (0.31)</td>\n",
       "      <td>favorite (0.32)</td>\n",
       "      <td>creator (0.36)</td>\n",
       "      <td>whale (0.37)</td>\n",
       "      <td>loosely (0.22)</td>\n",
       "      <td>nasty (0.37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lot (1.51)</td>\n",
       "      <td>series (1.22)</td>\n",
       "      <td>human (0.6)</td>\n",
       "      <td>tell (0.96)</td>\n",
       "      <td>use (0.76)</td>\n",
       "      <td>looking (0.98)</td>\n",
       "      <td>cinematography (0.6)</td>\n",
       "      <td>came (0.81)</td>\n",
       "      <td>call (0.57)</td>\n",
       "      <td>belief (0.47)</td>\n",
       "      <td>...</td>\n",
       "      <td>somewhat (0.32)</td>\n",
       "      <td>wondering (0.39)</td>\n",
       "      <td>mistake (0.36)</td>\n",
       "      <td>brevity (0.23)</td>\n",
       "      <td>justice (0.27)</td>\n",
       "      <td>history (0.32)</td>\n",
       "      <td>carver (0.23)</td>\n",
       "      <td>reptile (0.33)</td>\n",
       "      <td>randomly (0.21)</td>\n",
       "      <td>tough (0.36)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               36             62               48                23  \\\n",
       "0     like (2.45)     one (1.93)    rating (2.46)       watch (2.2)   \n",
       "1     ever (2.38)    show (1.92)     effect (1.3)        see (1.93)   \n",
       "2   really (2.21)    time (1.74)      main (0.87)        say (1.86)   \n",
       "3      even (2.1)    cast (1.62)       low (0.86)       look (1.64)   \n",
       "4    thing (2.09)   first (1.59)   overall (0.84)  recommend (1.15)   \n",
       "5     much (1.77)     end (1.57)  standard (0.66)       know (1.14)   \n",
       "6   people (1.74)    least (1.5)    single (0.66)       mean (1.05)   \n",
       "7  nothing (1.62)    hour (1.48)     along (0.64)  understand (1.0)   \n",
       "8    think (1.59)     back (1.3)     light (0.62)        sit (0.99)   \n",
       "9      lot (1.51)  series (1.22)      human (0.6)       tell (0.96)   \n",
       "\n",
       "            10              34                    41              86  \\\n",
       "0   get (1.88)    would (1.95)          movie (6.27)      got (1.58)   \n",
       "1  make (1.87)    could (1.78)           film (4.14)      saw (1.09)   \n",
       "2    go (1.76)      try (1.28)          scene (3.01)   seemed (1.06)   \n",
       "3  find (1.36)     want (1.26)         viewer (1.15)   played (0.96)   \n",
       "4  save (1.17)   enough (1.26)            gore (1.1)     gave (0.95)   \n",
       "5  give (1.12)     must (1.16)          flick (1.06)     went (0.93)   \n",
       "6  take (0.97)   please (1.11)         screen (0.91)  watched (0.92)   \n",
       "7  turn (0.91)   trying (1.06)           imdb (0.86)     left (0.86)   \n",
       "8  keep (0.78)     need (1.02)         sequel (0.81)     lost (0.84)   \n",
       "9   use (0.76)  looking (0.98)  cinematography (0.6)     came (0.81)   \n",
       "\n",
       "                26               58  ...                 98                30  \\\n",
       "0    action (1.78)      idea (1.34)  ...      pretty (1.61)      bored (1.11)   \n",
       "1   comment (1.32)      work (1.24)  ...     totally (1.06)      sorry (0.87)   \n",
       "2      case (1.29)      part (1.09)  ...  completely (0.95)      tired (0.79)   \n",
       "3    review (1.21)  direction (0.9)  ...       quite (0.82)   confused (0.62)   \n",
       "4  dialogue (1.16)   premise (0.88)  ...       truly (0.82)       hated (0.6)   \n",
       "5    budget (0.99)      role (0.84)  ...  especially (0.46)       sick (0.57)   \n",
       "6   opinion (0.69)   example (0.71)  ...   extremely (0.46)    spoiled (0.43)   \n",
       "7     group (0.67)    effort (0.65)  ...      highly (0.38)      angry (0.42)   \n",
       "8       act (0.66)      view (0.61)  ...   generally (0.33)     scared (0.41)   \n",
       "9      call (0.57)    belief (0.47)  ...    somewhat (0.32)  wondering (0.39)   \n",
       "\n",
       "                      43                  64              3   \\\n",
       "0            lack (1.29)        sense (1.32)     life (1.39)   \n",
       "1         problem (1.21)       talent (1.14)    child (1.23)   \n",
       "2          reason (1.17)        charm (0.72)      care (0.9)   \n",
       "3            mess (0.73)  imagination (0.43)   family (0.79)   \n",
       "4  disappointment (0.56)      emotion (0.29)     live (0.49)   \n",
       "5       situation (0.49)      oddness (0.27)  kibbutz (0.45)   \n",
       "6         trouble (0.46)        sheer (0.27)   native (0.37)   \n",
       "7      difference (0.42)       spirit (0.27)  country (0.36)   \n",
       "8        question (0.39)          ego (0.23)   living (0.31)   \n",
       "9         mistake (0.36)      brevity (0.23)  justice (0.27)   \n",
       "\n",
       "                   8                 78               76                   6   \\\n",
       "0         name (1.28)      actor (2.25)    animal (0.88)        poorly (1.12)   \n",
       "1         star (1.07)        fan (1.25)  creature (0.79)         badly (0.75)   \n",
       "2         world (0.8)     writer (0.98)       fox (0.65)  relentlessly (0.69)   \n",
       "3  blockbuster (0.58)   producer (0.79)     chick (0.56)       quickly (0.61)   \n",
       "4        dream (0.42)       hero (0.72)       rat (0.46)         often (0.35)   \n",
       "5         hype (0.36)     critic (0.56)     spider (0.4)          self (0.28)   \n",
       "6       former (0.36)  godfather (0.39)    hunter (0.38)   ambiguously (0.23)   \n",
       "7    attention (0.34)   reviewer (0.37)      joey (0.37)       briefly (0.22)   \n",
       "8     favorite (0.32)    creator (0.36)     whale (0.37)       loosely (0.22)   \n",
       "9      history (0.32)     carver (0.23)   reptile (0.33)      randomly (0.21)   \n",
       "\n",
       "                  13  \n",
       "0       worse (1.22)  \n",
       "1        slow (0.87)  \n",
       "2     serious (0.77)  \n",
       "3         hot (0.66)  \n",
       "4     tedious (0.58)  \n",
       "5    confusing (0.5)  \n",
       "6  impossible (0.45)  \n",
       "7     extreme (0.39)  \n",
       "8       nasty (0.37)  \n",
       "9       tough (0.36)  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Many-to-many WMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a first attempt to do the flows from words between many documents, without first filtering using Gale-Shapeley. However, this proved too inefficient. As you can see looking at the CPU times, it is very slow even with extremely small samples and the time complexity is quadratic (or worse?), meaning it rapidly gets even worse as the sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 12.7 s, total: 2min 1s\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%time m2m_distances = WMDManyToMany(pos_docs[:20], neg_docs[:20],E,idx2word).get_distances(return_flow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 52s, sys: 13.5 s, total: 2min 5s\n",
      "Wall time: 51.4 s\n"
     ]
    }
   ],
   "source": [
    "%time m2m_distances_flow, wc_X1, wc_X2 = WMDManyToMany(pos_docs[:20],neg_docs[:20],E,idx2word).get_distances(return_flow = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'karen': 8.69223,\n",
       " 'wrenching': 8.31882,\n",
       " 'carpenter': 7.468960000000001,\n",
       " 'laughter': 7.467879999999999,\n",
       " 'liked': 6.864090000000003,\n",
       " 'mom': 6.791519999999999,\n",
       " 'gut': 6.759419999999999,\n",
       " 'love': 6.551409999999997,\n",
       " 'camp': 6.533080000000001,\n",
       " 'hr': 6.1393699999999995}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wc_X1.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hopper': 8.372459999999998,\n",
       " 'jake': 7.63837,\n",
       " 'movie': 7.267059999999995,\n",
       " 'film': 6.936379999999998,\n",
       " 'shakespeare': 5.99276,\n",
       " 'oddness': 5.53033,\n",
       " 'terrible': 4.943440000000001,\n",
       " 'parent': 4.751790000000001,\n",
       " 'actor': 4.672620000000001,\n",
       " 'bad': 4.430020000000002}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(wc_X2.items(), key=lambda item: item[1], reverse=True)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-fwmd",
   "language": "python",
   "name": "venv-fwmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

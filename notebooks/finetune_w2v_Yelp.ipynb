{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_dir = os.path.join(os.getcwd(), '..')\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikaelbrunila/Documents/Code/flow-wmd/venv-fwmd/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from flow_wmd.utils import *\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 35s, sys: 1min 35s, total: 5min 10s\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "PATH = \"../data/yelp_dataset/\"\n",
    "yelp_data = []\n",
    "r_dtypes = {\"review_id\":str,\n",
    "            \"user_id\":str,\n",
    "            \"business_id\":str,\n",
    "            \"stars\": np.int32, \n",
    "            \"date\":str,\n",
    "            \"text\":str,\n",
    "            \"useful\": np.int32, \n",
    "            \"funny\": np.int32,\n",
    "            \"cool\": np.int32}\n",
    "drop = ['review_id', 'user_id','useful', 'funny', 'cool']\n",
    "#query = \"date >= '2017-12-01'\"\n",
    "\n",
    "with open(f\"{PATH}yelp_academic_dataset_review.json\", \"r\") as f:\n",
    "    reader = pd.read_json(f, orient=\"records\", lines=True, dtype=r_dtypes, chunksize=1000)\n",
    "    for chunk in reader:\n",
    "        reduced_chunk = chunk.drop(columns=drop)\n",
    "        #.query(query)\n",
    "        yelp_data.append(reduced_chunk)\n",
    "    \n",
    "yelp_data = pd.concat(yelp_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451826"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160585, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_business = pd.read_json(f\"{PATH}yelp_academic_dataset_business.json\", orient=\"records\", lines=True)\n",
    "yelp_business.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30815, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_business = yelp_business[yelp_business.city.isin([\"Portland\", \"Atlanta\"])]\n",
    "yelp_business.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_merged = yelp_data.merge(yelp_business, on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451826, 17)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.9 ms, sys: 3.8 ms, total: 88.7 ms\n",
      "Wall time: 88.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences = yelp_merged.text.astype('str').tolist()\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 5 s, total: 2min 25s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "sentences_clean=[remove_stopwords(r, stopword_list, tokenizer) for r in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%%time \n",
    "\n",
    "sentences_clean=pd.Series(sentences_clean).apply(denoise_text)\n",
    "sentences_clean=sentences_clean.apply(remove_special_characters)\n",
    "sentences_clean=sentences_clean.apply(simple_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 2.36 s, total: 1min 23s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences_clean=[remove_stopwords(r, stopword_list, tokenizer) for r in sentences_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'even mad captain ahab touched starbuck goodness herman melville drivethru jammed vehicle pulled opted park go walkup window staff super friendly said hello ordered even though busy genuinely smiling look like enjoy job great teamwork think friendliest starbucks ever visited building architecturally pleasing catch eye every time driven past glad stopped time close freeway entrance making regular stop area'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.8 s, sys: 3.65 s, total: 41.5 s\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences_tokenized = [w.lower() for w in sentences_clean]\n",
    "sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n",
      "CPU times: user 52.4 s, sys: 4.68 s, total: 57.1 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phrase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASING = True\n",
    "MIN = 500\n",
    "THRESHOLD = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 8s, sys: 8.89 s, total: 4min 17s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if PHRASING:\n",
    "    sentences_phrased = get_phrases(sentences_tokenized, \n",
    "                                    min_count = MIN, \n",
    "                                    threshold = THRESHOLD)\n",
    "    sentences_training = sentences_phrased\n",
    "    \n",
    "else:\n",
    "    sentences_training = sentences_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'mad', 'captain', 'ahab', 'touched', 'starbuck', 'goodness', 'herman', 'melville', 'drivethru', 'jammed', 'vehicle', 'pulled', 'opted', 'park', 'go', 'walkup_window', 'staff', 'super_friendly', 'said_hello', 'ordered', 'even_though', 'busy', 'genuinely', 'smiling', 'look', 'like', 'enjoy', 'job', 'great', 'teamwork', 'think', 'friendliest', 'starbucks', 'ever', 'visited', 'building', 'architecturally', 'pleasing', 'catch_eye', 'every', 'time', 'driven_past', 'glad_stopped', 'time', 'close_freeway', 'entrance', 'making', 'regular', 'stop', 'area']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worker', 'absolute_sweetest', 'work', 'dollar_tree', 'close', 'get', 'coffee', 'work', 'always', 'super', 'kind', 'sweet', 'love', 'starbucks']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_training[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['super', 'slow', 'ive', 'never', 'waited', 'long_line', 'even', 'move', 'insane', 'better']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_training[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetune Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialize Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch {self.epoch} starting.\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f\"Epoch {self.epoch} ended.\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Output loss at each epoch'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch != 1:\n",
    "            previous_loss = self.losses[self.epoch-2]\n",
    "        else:\n",
    "            previous_loss = 0\n",
    "        self.losses.append(loss)\n",
    "        difference = loss-previous_loss\n",
    "        print(f'  Loss: {loss}  Difference: {difference}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logger = EpochLogger()\n",
    "loss_logger = LossLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = model.vector_size\n",
    "WINDOW = 10\n",
    "EPOCHS = 4\n",
    "MIN_COUNT = 2\n",
    "SG = 1\n",
    "HS = 0\n",
    "SEED = 42\n",
    "LOSS = True\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.3 s, sys: 311 ms, total: 9.61 s\n",
      "Wall time: 9.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_ft = Word2Vec(vector_size= SIZE, \n",
    "                    window = WINDOW,\n",
    "                    min_count= MIN_COUNT,\n",
    "                    epochs=EPOCHS,\n",
    "                    sg = SG,\n",
    "                    hs = HS,\n",
    "                    seed = SEED)\n",
    "model_ft.build_vocab(sentences_training)\n",
    "total_examples = model_ft.corpus_count\n",
    "model_ft.build_vocab([list(model.key_to_index.keys())], update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"../embeddings/yelp_w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t  Loss: 64311736.0  Difference: 64311736.0\n",
      "Epoch: 2\t  Loss: 68005744.0  Difference: 3694008.0\n",
      "Epoch: 3\t  Loss: 68840552.0  Difference: 834808.0\n",
      "Epoch: 4\t  Loss: 69504992.0  Difference: 664440.0\n",
      "CPU times: user 23min 46s, sys: 11.7 s, total: 23min 57s\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_ft.train(sentences_training, \n",
    "               total_examples=total_examples,\n",
    "               epochs=model_ft.epochs,\n",
    "               callbacks=[loss_logger],\n",
    "               compute_loss=LOSS,\n",
    "               start_alpha = ALPHA)\n",
    "model_ft.wv.save_word2vec_format(f\"{outfile}.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Load Finetuned Vectors and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(f\"{outfile}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45544904470443726"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vectors.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9603805989027023"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29060083627700806"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vectors.distance(\"lord\", \"ring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7185895442962646"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"lord\", \"ring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03295106, -0.10795593, -0.10198571,  0.00111192,  0.00233896,\n",
       "       -0.06005668,  0.10686377,  0.03378343, -0.08767647,  0.02884012],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vectors.get_vector(\"citizen_kane\")[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-fwmd",
   "language": "python",
   "name": "venv-fwmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

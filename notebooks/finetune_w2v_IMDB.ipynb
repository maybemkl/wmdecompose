{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "data = pd.read_csv(f\"{PATH}IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.review.astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_tokenized = [w.lower() for w in sentences]\n",
    "sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phrase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(sentences, min_count=5, threshold=100):\n",
    "    bigram = Phrases(sentences, min_count=min_count, threshold = threshold) # higher threshold fewer phrases.\n",
    "    trigram = Phrases(bigram[sentences])  \n",
    "\n",
    "    # 'Phraser' is a wrapper that makes 'Phrases' run faster\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    trigram_phraser = Phraser(trigram)\n",
    "\n",
    "    phrased_bi = [b for b in bigram[sentences]]\n",
    "    phrased_tri = [t for t in trigram[[b for b in bigram[sentences]]]]\n",
    "    \n",
    "    return phrased_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASING = True\n",
    "MIN = 5\n",
    "THRESHOLD = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "if PHRASING:\n",
    "    sentences_phrased = get_phrases(sentences_tokenized, \n",
    "                                    min_count = MIN, \n",
    "                                    threshold = THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetune Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialize Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch {self.epoch} starting.\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f\"Epoch {self.epoch} ended.\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Output loss at each epoch'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch != 1:\n",
    "            previous_loss = self.losses[self.epoch-2]\n",
    "        else:\n",
    "            previous_loss = 0\n",
    "        self.losses.append(loss)\n",
    "        difference = loss-previous_loss\n",
    "        print(f'  Loss: {loss}  Difference: {difference}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logger = EpochLogger()\n",
    "loss_logger = LossLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = model.vector_size\n",
    "WINDOW = 10\n",
    "EPOCHS = 10\n",
    "MIN_COUNT = 2\n",
    "SG = 1\n",
    "HS = 0\n",
    "SEED = 42\n",
    "LOSS = True\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "model_ft = Word2Vec(vector_size= SIZE, \n",
    "                    window = WINDOW,\n",
    "                    min_count= MIN_COUNT,\n",
    "                    epochs=EPOCHS,\n",
    "                    sg = SG,\n",
    "                    hs = HS,\n",
    "                    seed = SEED)\n",
    "model_ft.build_vocab(sentences_tokenized)\n",
    "total_examples = model_ft.corpus_count\n",
    "model_ft.build_vocab([list(model.key_to_index.keys())], update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"../embeddings/imdb_w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "model_ft.train(sentences_tokenized, \n",
    "               total_examples=total_examples,\n",
    "               epochs=model_ft.epochs,\n",
    "               callbacks=[loss_logger],\n",
    "               compute_loss=LOSS,\n",
    "               start_alpha = ALPHA)\n",
    "model_ft.wv.save_word2vec_format(f\"{outfile}.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Load Finetuned Vectors and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(f\"{outfile}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-fwmd",
   "language": "python",
   "name": "venv-fwmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

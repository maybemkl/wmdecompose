{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "data = pd.read_csv(f\"{PATH}IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.review.astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_tokenized = [w.lower() for w in sentences]\n",
    "sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n",
      "CPU times: user 48.5 s, sys: 4.09 s, total: 52.6 s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phrase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(sentences, min_count=5, threshold=100):\n",
    "    bigram = Phrases(sentences, min_count=min_count, threshold = threshold) # higher threshold fewer phrases.\n",
    "    trigram = Phrases(bigram[sentences])  \n",
    "\n",
    "    # 'Phraser' is a wrapper that makes 'Phrases' run faster\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    trigram_phraser = Phraser(trigram)\n",
    "\n",
    "    phrased_bi = [b for b in bigram[sentences]]\n",
    "    phrased_tri = [t for t in trigram[[b for b in bigram[sentences]]]]\n",
    "    \n",
    "    return phrased_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASING = True\n",
    "MIN = 10\n",
    "THRESHOLD = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "if PHRASING:\n",
    "    sentences_phrased = get_phrases(sentences_tokenized, \n",
    "                                    min_count = MIN, \n",
    "                                    threshold = THRESHOLD)\n",
    "    sentences_training = sentences_phrased\n",
    "    \n",
    "else:\n",
    "    sentences_training = sentences_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'of', 'the', 'other_reviewers', 'has', 'mentioned', 'that', 'after_watching', 'just', '1', 'oz', 'episode', 'you_ll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly_what', 'happened', 'with', 'me', 'br', 'br', 'the', 'first', 'thing', 'that', 'struck_me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust_me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint_hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no_punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'br', 'br', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum_security', 'state', 'penitentary', 'it', 'focuses_mainly', 'on', 'emerald_city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far_away', 'br', 'br', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn_t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream_audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn_t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck_me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn_t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic_violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well_mannered', 'middle_class', 'inmates', 'being', 'turned_into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if_you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker_side']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_training[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetune Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialize Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch {self.epoch} starting.\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f\"Epoch {self.epoch} ended.\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Output loss at each epoch'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch != 1:\n",
    "            previous_loss = self.losses[self.epoch-2]\n",
    "        else:\n",
    "            previous_loss = 0\n",
    "        self.losses.append(loss)\n",
    "        difference = loss-previous_loss\n",
    "        print(f'  Loss: {loss}  Difference: {difference}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logger = EpochLogger()\n",
    "loss_logger = LossLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = model.vector_size\n",
    "WINDOW = 10\n",
    "EPOCHS = 4\n",
    "MIN_COUNT = 2\n",
    "SG = 1\n",
    "HS = 0\n",
    "SEED = 42\n",
    "LOSS = True\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "model_ft = Word2Vec(vector_size= SIZE, \n",
    "                    window = WINDOW,\n",
    "                    min_count= MIN_COUNT,\n",
    "                    epochs=EPOCHS,\n",
    "                    sg = SG,\n",
    "                    hs = HS,\n",
    "                    seed = SEED)\n",
    "model_ft.build_vocab(sentences_training)\n",
    "total_examples = model_ft.corpus_count\n",
    "model_ft.build_vocab([list(model.key_to_index.keys())], update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"../embeddings/imdb_w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n",
      "Epoch: 1\t  Loss: 43901376.0  Difference: 43901376.0\n",
      "Epoch: 2\t  Loss: 67113664.0  Difference: 23212288.0\n",
      "Epoch: 3\t  Loss: 67149352.0  Difference: 35688.0\n",
      "Epoch: 4\t  Loss: 67188464.0  Difference: 39112.0\n",
      "Epoch: 5\t  Loss: 67230064.0  Difference: 41600.0\n",
      "Epoch: 6\t  Loss: 67273912.0  Difference: 43848.0\n",
      "Epoch: 7\t  Loss: 67319536.0  Difference: 45624.0\n",
      "Epoch: 8\t  Loss: 67365376.0  Difference: 45840.0\n",
      "Epoch: 9\t  Loss: 67410608.0  Difference: 45232.0\n",
      "Epoch: 10\t  Loss: 67454848.0  Difference: 44240.0\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "model_ft.train(sentences_training, \n",
    "               total_examples=total_examples,\n",
    "               epochs=model_ft.epochs,\n",
    "               callbacks=[loss_logger],\n",
    "               compute_loss=LOSS,\n",
    "               start_alpha = ALPHA)\n",
    "model_ft.wv.save_word2vec_format(f\"{outfile}.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Load Finetuned Vectors and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(f\"{outfile}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5875533521175385"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vectors.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9603805989027023"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01757576, -0.12813944,  0.04717635, -0.15354194,  0.03044946,\n",
       "       -0.07767139,  0.10965573,  0.03233631, -0.21190403,  0.21741547,\n",
       "        0.03698666,  0.12758844,  0.00124411,  0.05378597,  0.04423649,\n",
       "       -0.19151859, -0.02145447, -0.15648721, -0.11356307, -0.02502821,\n",
       "        0.12017536,  0.11932378, -0.10764558,  0.20080061,  0.08048956,\n",
       "        0.08712426, -0.31180596, -0.01730541,  0.06941913,  0.03797434,\n",
       "       -0.13839902, -0.03498537,  0.02344241, -0.13848041,  0.00510201,\n",
       "        0.00162221,  0.09475488, -0.12750952, -0.02189669,  0.03248203,\n",
       "        0.08265379, -0.005041  ,  0.04969675,  0.14311759, -0.00120467,\n",
       "        0.02637135,  0.19980508,  0.17875028,  0.02458126,  0.04515405,\n",
       "       -0.11245415, -0.02185427, -0.04695068,  0.01496219,  0.19356494,\n",
       "       -0.08108859,  0.01949373,  0.09136099, -0.03426363,  0.06105224,\n",
       "        0.03971599, -0.01731453, -0.01453782,  0.09799112,  0.00626202,\n",
       "        0.0805092 ,  0.11567568,  0.08519507, -0.1119812 ,  0.12581035,\n",
       "       -0.04558104, -0.10114152, -0.00474344,  0.10187408, -0.0139407 ,\n",
       "        0.01887074, -0.08266993,  0.04939166, -0.03725989,  0.00701609,\n",
       "        0.02014825, -0.09661156,  0.03223231, -0.01430645, -0.02874938,\n",
       "       -0.02747321,  0.02382682, -0.10832705, -0.01587791, -0.07532031,\n",
       "       -0.09007055, -0.04974574, -0.01169786, -0.03386014, -0.01224371,\n",
       "       -0.05565527, -0.10602736,  0.1078378 ,  0.00037764, -0.00204522,\n",
       "        0.01379404,  0.00784791, -0.0316015 ,  0.07063247, -0.21903338,\n",
       "        0.04789446,  0.02749022,  0.06841202,  0.06687073, -0.14486939,\n",
       "       -0.12783413,  0.04220395, -0.08569524,  0.16031158, -0.17938226,\n",
       "        0.05508675,  0.02912297,  0.0598583 , -0.16562264,  0.08257483,\n",
       "        0.0320605 ,  0.11557035,  0.06025576,  0.02333012,  0.06407138,\n",
       "       -0.06852317,  0.05482938,  0.10370151, -0.04191345,  0.02604607,\n",
       "       -0.00908927,  0.02408893,  0.12543914,  0.05845254,  0.09364747,\n",
       "        0.00460238,  0.07692135, -0.09696045,  0.13147207,  0.13598964,\n",
       "        0.09848224, -0.02796948,  0.12563269, -0.02194333,  0.05897218,\n",
       "        0.06040944,  0.00868668,  0.04885753,  0.0562805 , -0.19877125,\n",
       "        0.05078663, -0.35236424, -0.02844503, -0.08359987,  0.03333503,\n",
       "        0.04547791, -0.05346266,  0.07058199,  0.09878019, -0.04349119,\n",
       "       -0.06169074, -0.07916822, -0.04934982, -0.06008434, -0.05879257,\n",
       "        0.0409121 ,  0.01771445, -0.07211377, -0.02120269, -0.19449505,\n",
       "       -0.05770239,  0.16823238, -0.1018389 ,  0.05204668,  0.05181611,\n",
       "       -0.01753692,  0.01123791, -0.11680866,  0.03017112, -0.00494367,\n",
       "        0.08160147, -0.11108898, -0.17397977, -0.0086442 ,  0.02098906,\n",
       "        0.06647411,  0.08303521, -0.13386922, -0.10390989,  0.01837318,\n",
       "       -0.08369444,  0.13360846,  0.01230045, -0.09655628, -0.01781503,\n",
       "       -0.1115426 , -0.14102203,  0.07166769, -0.15735191, -0.10228301,\n",
       "        0.08482151, -0.03206772, -0.0570855 ,  0.08615641,  0.0373034 ,\n",
       "        0.13198861,  0.12912315,  0.00248683, -0.00527668,  0.0193838 ,\n",
       "       -0.06631982, -0.1199343 ,  0.0037724 ,  0.16858447, -0.03324115,\n",
       "       -0.07215438, -0.11352739, -0.03965265,  0.20177428,  0.17961425,\n",
       "       -0.18127073, -0.01909694, -0.03867536, -0.00701537, -0.03395832,\n",
       "       -0.0724794 ,  0.06979802,  0.08617888, -0.0391087 , -0.0591758 ,\n",
       "        0.08312558, -0.09560869, -0.09973747,  0.00246673,  0.01142589,\n",
       "       -0.01685439,  0.048573  ,  0.07022206, -0.06695899, -0.2672224 ,\n",
       "       -0.1338556 ,  0.05827991,  0.01805337, -0.00988807, -0.03689525,\n",
       "       -0.00566085,  0.12869865,  0.12281565,  0.07700499, -0.02180702,\n",
       "       -0.1099263 ,  0.00515598, -0.17583853,  0.0161298 , -0.06728337,\n",
       "        0.00211974, -0.09801875, -0.06944299,  0.06083606,  0.13532798,\n",
       "        0.08286386,  0.00041748, -0.09149982,  0.06794974,  0.20879278,\n",
       "       -0.03098843,  0.01260233, -0.0606631 , -0.18564151,  0.0467328 ,\n",
       "        0.05412784, -0.04727447, -0.01562996, -0.13185741,  0.15907323,\n",
       "       -0.03721077,  0.08167885,  0.00959191, -0.0323809 , -0.09976204,\n",
       "       -0.0289126 , -0.09537456, -0.05338591, -0.14244485, -0.02954094,\n",
       "        0.00590628,  0.05353665, -0.00843766, -0.06534971,  0.04345378,\n",
       "       -0.09379391,  0.10790971,  0.01199956,  0.04570037, -0.03228039,\n",
       "        0.11334203,  0.21771169,  0.14615369, -0.06616864,  0.01128549],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vectors.get_vector(\"faint_hearted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-fwmd",
   "language": "python",
   "name": "venv-fwmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

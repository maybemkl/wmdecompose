{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\"\n",
    "data = pd.read_csv(f\"{PATH}IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.review.astype('str').tolist()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_tokenized = [w.lower() for w in sentences]\n",
    "sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GoogleNews Vectors\n",
      "CPU times: user 48.5 s, sys: 4.09 s, total: 52.6 s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GoogleNews Vectors\")\n",
    "%time model = KeyedVectors.load_word2vec_format('../embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phrase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(sentences, min_count=5, threshold=100):\n",
    "    bigram = Phrases(sentences, min_count=min_count, threshold = threshold) # higher threshold fewer phrases.\n",
    "    trigram = Phrases(bigram[sentences])  \n",
    "\n",
    "    # 'Phraser' is a wrapper that makes 'Phrases' run faster\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    trigram_phraser = Phraser(trigram)\n",
    "\n",
    "    phrased_bi = [b for b in bigram[sentences]]\n",
    "    phrased_tri = [t for t in trigram[[b for b in bigram[sentences]]]]\n",
    "    \n",
    "    return phrased_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASING = True\n",
    "MIN = 10\n",
    "THRESHOLD = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "if PHRASING:\n",
    "    sentences_phrased = get_phrases(sentences_tokenized, \n",
    "                                    min_count = MIN, \n",
    "                                    threshold = THRESHOLD)\n",
    "    sentences_training = sentences_phrased\n",
    "    \n",
    "else:\n",
    "    sentences_training = sentences_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'of', 'the', 'other_reviewers', 'has', 'mentioned', 'that', 'after_watching', 'just', '1', 'oz', 'episode', 'you_ll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly_what', 'happened', 'with', 'me', 'br', 'br', 'the', 'first', 'thing', 'that', 'struck_me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust_me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint_hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no_punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'br', 'br', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum_security', 'state', 'penitentary', 'it', 'focuses_mainly', 'on', 'emerald_city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far_away', 'br', 'br', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn_t', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream_audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn_t', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck_me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn_t', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic_violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well_mannered', 'middle_class', 'inmates', 'being', 'turned_into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if_you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker_side']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_training[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetune Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Initialize Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch {self.epoch} starting.\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f\"Epoch {self.epoch} ended.\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Output loss at each epoch'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch != 1:\n",
    "            previous_loss = self.losses[self.epoch-2]\n",
    "        else:\n",
    "            previous_loss = 0\n",
    "        self.losses.append(loss)\n",
    "        difference = loss-previous_loss\n",
    "        print(f'  Loss: {loss}  Difference: {difference}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logger = EpochLogger()\n",
    "loss_logger = LossLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = model.vector_size\n",
    "WINDOW = 10\n",
    "EPOCHS = 10\n",
    "MIN_COUNT = 2\n",
    "SG = 1\n",
    "HS = 0\n",
    "SEED = 42\n",
    "LOSS = True\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "model_ft = Word2Vec(vector_size= SIZE, \n",
    "                    window = WINDOW,\n",
    "                    min_count= MIN_COUNT,\n",
    "                    epochs=EPOCHS,\n",
    "                    sg = SG,\n",
    "                    hs = HS,\n",
    "                    seed = SEED)\n",
    "model_ft.build_vocab(sentences_training)\n",
    "total_examples = model_ft.corpus_count\n",
    "model_ft.build_vocab([list(model.key_to_index.keys())], update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = \"../embeddings/imdb_w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n",
      "Epoch: 1\t  Loss: 43901376.0  Difference: 43901376.0\n",
      "Epoch: 2\t  Loss: 67113664.0  Difference: 23212288.0\n",
      "Epoch: 3\t  Loss: 67149352.0  Difference: 35688.0\n",
      "Epoch: 4\t  Loss: 67188464.0  Difference: 39112.0\n",
      "Epoch: 5\t  Loss: 67230064.0  Difference: 41600.0\n",
      "Epoch: 6\t"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "model_ft.train(sentences_training, \n",
    "               total_examples=total_examples,\n",
    "               epochs=model_ft.epochs,\n",
    "               callbacks=[loss_logger],\n",
    "               compute_loss=LOSS,\n",
    "               start_alpha = ALPHA)\n",
    "model_ft.wv.save_word2vec_format(f\"{outfile}.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Load Finetuned Vectors and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(f\"{outfile}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance(\"citizen\", \"kane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vectors.get_vector(\"faint_hearted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-fwmd",
   "language": "python",
   "name": "venv-fwmd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
